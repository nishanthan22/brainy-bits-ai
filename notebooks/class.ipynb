{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc756ef-3f19-4a9b-afcb-89c0dd1d6e03",
   "metadata": {},
   "source": [
    "!pip install cv2\n",
    "\n",
    "import cv2\n",
    "import dlib\n",
    "import numpy as np\n",
    "from IPython.display import display, clear_output\n",
    "from scipy.spatial import distance\n",
    "from imutils import face_utils\n",
    "from scipy.ndimage import zoom\n",
    "from tensorflow.keras.models import load_model\n",
    "import mediapipe as mp\n",
    "import time\n",
    "import csv\n",
    "\n",
    "# Function to detect eyes in a frame\n",
    "def detect_eyes(frame):\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = detector(gray)\n",
    "\n",
    "    if faces:\n",
    "        shape = predictor(gray, faces[0])\n",
    "        left_eye = shape.parts()[36:42]\n",
    "        right_eye = shape.parts()[42:48]\n",
    "        return left_eye, right_eye\n",
    "    else:\n",
    "        return None, None\n",
    "\n",
    "# Function to calculate Eye Aspect Ratio (EAR)\n",
    "def calculate_ear(eye):\n",
    "    eye = np.array([(point.x, point.y) for point in eye])\n",
    "    A = np.linalg.norm(eye[1] - eye[5])\n",
    "    B = np.linalg.norm(eye[2] - eye[4])\n",
    "    C = np.linalg.norm(eye[0] - eye[3])\n",
    "    ear = (A + B) / (2.0 * C)\n",
    "    return ear\n",
    "\n",
    "# Load dlib face detector and facial landmarks predictor\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"C:/Users/nisha/Downloads/brainy-bits/brainy-bits-ai/notebooks/shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "# Load emotion detection model\n",
    "emotion_model = load_model(\"C:/Users/nisha/Downloads/brainy-bits/brainy-bits-ai/notebooks/video.h5\")\n",
    "\n",
    "# Initialize video capture from the camera\n",
    "cap = cv2.VideoCapture(0)  # 0 corresponds to the default camera (you can change it if you have multiple cameras)\n",
    "\n",
    "# Get video properties for the output video\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# Initialize video writer for the output video\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "output_video = cv2.VideoWriter('/content/outputvideo.avi', fourcc, fps, (width, height))\n",
    "\n",
    "# Initialize variables to record durations\n",
    "duration_eyes_closed = 0\n",
    "duration_looking_left = 0\n",
    "duration_looking_right = 0\n",
    "duration_looking_straight = 0\n",
    "\n",
    "# Initialize variables for counting eye movement\n",
    "count_left = 0\n",
    "count_right = 0\n",
    "count_straight = 0\n",
    "\n",
    "# Load face detector and shape predictor for emotion detection\n",
    "face_detector = dlib.get_frontal_face_detector()\n",
    "shape_predictor = dlib.shape_predictor(\"C:/Users/nisha/Downloads/brainy-bits/brainy-bits-ai/notebooks/face_landmarks.dat\")\n",
    "\n",
    "# Initialize head pose estimation\n",
    "official_start_time = time.time()\n",
    "start_time = time.time()\n",
    "end_time = 0\n",
    "\n",
    "#Variables to track emotion detected\n",
    "emotion_start_time = time.time()\n",
    "e_start_time = time.time()\n",
    "e_end_time = 0\n",
    "angry_emotion = 0\n",
    "sad_emotion = 0\n",
    "happy_emotion = 0\n",
    "fear_emotion = 0\n",
    "disgust_emotion = 0\n",
    "neutral_emotion = 0\n",
    "surprise_emotion = 0\n",
    "\n",
    "# Variables to track time spent in different head pose directions\n",
    "time_forward_seconds = 0\n",
    "time_left_seconds = 0\n",
    "time_right_seconds = 0\n",
    "time_up_seconds = 0\n",
    "time_down_seconds = 0\n",
    "\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Eye tracking\n",
    "    left_eye, right_eye = detect_eyes(frame)\n",
    "\n",
    "    if left_eye is not None and right_eye is not None:\n",
    "        ear_left = calculate_ear(left_eye)\n",
    "        ear_right = calculate_ear(right_eye)\n",
    "\n",
    "        # Calculate the average EAR for both eyes\n",
    "        avg_ear = (ear_left + ear_right) / 2.0\n",
    "\n",
    "        # Set a threshold for distraction detection (you may need to adjust this)\n",
    "        distraction_threshold = 0.2\n",
    "\n",
    "        # Check if the person is distracted\n",
    "        if avg_ear < distraction_threshold:\n",
    "            cv2.putText(frame, \"Eyes Closed\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 0, 255), 2)\n",
    "            duration_eyes_closed += 1 / fps  # Increment the duration\n",
    "            count_straight += 1\n",
    "\n",
    "        else:\n",
    "            # Check gaze direction\n",
    "            horizontal_ratio = (left_eye[0].x + right_eye[3].x) / 2 / width\n",
    "            if horizontal_ratio < 0.4:\n",
    "                cv2.putText(frame, \"Looking Left\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 0), 2)\n",
    "                duration_looking_left += 1 / fps  # Increment the duration\n",
    "                count_left += 1\n",
    "            elif horizontal_ratio > 0.6:\n",
    "                cv2.putText(frame, \"Looking Right\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 0), 2)\n",
    "                duration_looking_right += 1 / fps  # Increment the duration\n",
    "                count_right += 1\n",
    "            else:\n",
    "                cv2.putText(frame, \"Looking Straight\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 0), 2)\n",
    "                duration_looking_straight += 1 / fps  # Increment the duration\n",
    "\n",
    "        # Draw contours around eyes\n",
    "        for eye in [left_eye, right_eye]:\n",
    "            for point in eye:\n",
    "                x, y = point.x, point.y\n",
    "                cv2.circle(frame, (x, y), 3, (0, 255, 0), -1)\n",
    "\n",
    "    # Emotion detection\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    rects = face_detector(gray, 1)\n",
    "\n",
    "    for (i, rect) in enumerate(rects):\n",
    "        shape = shape_predictor(gray, rect)\n",
    "        shape = face_utils.shape_to_np(shape)\n",
    "\n",
    "        (x, y, w, h) = face_utils.rect_to_bb(rect)\n",
    "        face = gray[y:y + h, x:x + w]\n",
    "        face = zoom(face, (48 / face.shape[0], 48 / face.shape[1]))\n",
    "        face = face.astype(np.float32)\n",
    "        face /= float(face.max())\n",
    "        face = np.reshape(face.flatten(), (1, 48, 48, 1))\n",
    "\n",
    "        prediction = emotion_model.predict(face)\n",
    "        prediction_result = np.argmax(prediction)\n",
    "\n",
    "        # Rectangle around the face\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "        # Annotate main image with emotion label\n",
    "        if prediction_result == 0:\n",
    "            cv2.putText(frame, \"Angry\", (x + w - 10, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            angry_emotion += time.time() - e_start_time\n",
    "            e_start_time = time.time()\n",
    "        elif prediction_result == 1:\n",
    "            cv2.putText(frame, \"Disgust\", (x + w - 10, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            disgust_emotion += time.time() - e_start_time\n",
    "            e_start_time = time.time()\n",
    "        elif prediction_result == 2:\n",
    "            cv2.putText(frame, \"Fear\", (x + w - 10, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            fear_emotion += time.time() - e_start_time \n",
    "            e_start_time = time.time()\n",
    "        elif prediction_result == 3:\n",
    "            cv2.putText(frame, \"Happy\", (x + w - 10, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            happy_emotion += time.time() - e_start_time \n",
    "            e_start_time = time.time()\n",
    "        elif prediction_result == 4:\n",
    "            cv2.putText(frame, \"Sad\", (x + w - 10, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            sad_emotion += time.time() - e_start_time \n",
    "            e_start_time = time.time()\n",
    "        elif prediction_result == 5:\n",
    "            cv2.putText(frame, \"Surprise\", (x + w - 10, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            surprise_emotion += time.time() - e_start_time \n",
    "            e_start_time = time.time()\n",
    "        else:\n",
    "            cv2.putText(frame, \"Neutral\", (x + w - 10, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            neutral_emotion += time.time() - e_start_time \n",
    "            e_start_time = time.time()\n",
    "\n",
    "    # Head pose estimation\n",
    "    startTime = time.time()\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) #it was 1\n",
    "#     frame = cv2.cvtColor(cv2.flip(frame, 1), cv2.COLOR_BGR2RGB)\n",
    "    frame.flags.writeable = False\n",
    "    results = face_mesh.process(frame)\n",
    "    frame.flags.writeable = True\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    img_h, img_w, img_c = frame.shape\n",
    "    face_3d = []\n",
    "    face_2d = []\n",
    "\n",
    "    if results.multi_face_landmarks:\n",
    "        for face_landmarks in results.multi_face_landmarks:\n",
    "            for idx, lm in enumerate(face_landmarks.landmark):\n",
    "                if idx == 33 or idx == 263 or idx == 1 or idx == 61 or idx == 291 or idx == 199:\n",
    "                    if idx == 1:\n",
    "                        nose_2d = (lm.x * img_w, lm.y * img_h)\n",
    "                        nose_3d = (lm.x * img_w, lm.y * img_h, lm.z * 8000)\n",
    "\n",
    "                    x, y = int(lm.x * img_w), int(lm.y * img_h)\n",
    "\n",
    "                    # Get the 2D Coordinates\n",
    "                    face_2d.append([x, y])\n",
    "\n",
    "                    # Get the 3D Coordinates\n",
    "                    face_3d.append([x, y, lm.z])\n",
    "\n",
    "            face_2d = np.array(face_2d, dtype=np.float64)\n",
    "            face_3d = np.array(face_3d, dtype=np.float64)\n",
    "            focal_length = 1 * img_w\n",
    "\n",
    "            cam_matrix = np.array([[focal_length, 0, img_h / 2],\n",
    "                                   [0, focal_length, img_w / 2],\n",
    "                                   [0, 0, 1]])\n",
    "\n",
    "            dist_matrix = np.zeros((4, 1), dtype=np.float64)\n",
    "            success, rot_vec, trans_vec = cv2.solvePnP(face_3d, face_2d, cam_matrix, dist_matrix)\n",
    "            rmat, jac = cv2.Rodrigues(rot_vec)\n",
    "\n",
    "            angles, mtxR, mtxQ, Qx, Qy, Qz = cv2.RQDecomp3x3(rmat)\n",
    "\n",
    "            print(f\"X Rotation: {angles[0] * 10000}\")\n",
    "            print(f\"Y Rotation: {angles[1] * 10000}\")\n",
    "\n",
    "            if angles[1] * 10000 < -100:\n",
    "                text = \"Looking Left\"\n",
    "                time_left_seconds += time.time() - start_time\n",
    "                start_time = time.time()\n",
    "\n",
    "            elif angles[1] * 10000 > 100:\n",
    "                text = \"Looking Right\"\n",
    "                time_right_seconds += time.time() - start_time\n",
    "                start_time = time.time()\n",
    "\n",
    "            elif angles[0] * 10000 < -100:\n",
    "                text = \"Looking Down\"\n",
    "                time_down_seconds += time.time() - start_time\n",
    "                start_time = time.time()\n",
    "\n",
    "            elif angles[0] * 10000 > 200:\n",
    "                text = \"Looking Up\"\n",
    "                time_up_seconds += time.time() - start_time\n",
    "                start_time = time.time()\n",
    "\n",
    "            else:\n",
    "                text = \"Forward\"\n",
    "                time_forward_seconds += time.time() - start_time\n",
    "                start_time = time.time()\n",
    "\n",
    "            # Display the nose direction\n",
    "            nose_3d_projection, jacobian = cv2.projectPoints(nose_3d, rot_vec, trans_vec, cam_matrix, dist_matrix)\n",
    "\n",
    "            p1 = (int(nose_2d[0]), int(nose_2d[1]))\n",
    "            p2 = (int(nose_3d_projection[0][0][0]), int(nose_3d_projection[0][0][1]))\n",
    "\n",
    "            cv2.line(frame, p1, p2, (255, 0, 0), 2)\n",
    "\n",
    "            cv2.putText(frame, text, (width - 250, 30), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 0, 255), 2)\n",
    "\n",
    "    # Open the CSV file in write mode and append the angles to it\n",
    "    with open('headPoses.csv', mode='a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "\n",
    "        # Write the header row if the file is empty\n",
    "        if file.tell() == 0:\n",
    "            writer.writerow([\"X Rotation\", \"Y Rotation\"])\n",
    "\n",
    "        # Write the angles to the CSV file\n",
    "        #writer.writerow([angles[0] * 10000, angles[1] * 10000]) #bonbon\n",
    "\n",
    "    output_video.write(frame)  # Write the frame to the output video\n",
    "\n",
    "    # Display the frame without modifying color\n",
    "    cv2.imshow('Frame', frame)\n",
    "    # Clear the previous output\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture object, video writer, and close all windows\n",
    "cap.release()\n",
    "output_video.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "# Print the durations and most observed features for emotion detection\n",
    "print(f\"\\nEmotion Detection:\")\n",
    "print(f\"Duration of Happiness: {happy_emotion} seconds\")\n",
    "print(f\"Duration of Sadness: {sad_emotion} seconds\")\n",
    "print(f\"Duration of Disgust: {disgust_emotion} seconds\")\n",
    "print(f\"Duration of Fear: {fear_emotion} seconds\")\n",
    "print(f\"Duration of Anger: {angry_emotion} seconds\")\n",
    "print(f\"Duration of Neutral: {neutral_emotion} seconds\")\n",
    "print(f\"Duration of Surprise: {surprise_emotion} seconds\")\n",
    "\n",
    "# Determine the most observed emotions movement\n",
    "max_eye_duration = max(happy_emotion, sad_emotion, disgust_emotion, fear_emotion, angry_emotion, neutral_emotion, surprise_emotion)\n",
    "if max_eye_duration == happy_emotion:\n",
    "    print(\"The most observed emotion: Happiness\")\n",
    "elif max_eye_duration == sad_emotion:\n",
    "    print(\"The most observed emotion: Sadness\")\n",
    "elif max_eye_duration == disgust_emotion:\n",
    "    print(\"The most observed emotion: Disgust\")\n",
    "elif max_eye_duration == fear_emotion:\n",
    "    print(\"The most observed emotion: Fear\")\n",
    "elif max_eye_duration == angry_emotion:\n",
    "    print(\"The most observed emotion: Anger\")\n",
    "elif max_eye_duration == surprise_emotion:\n",
    "    print(\"The most observed emotion: Surprise\")\n",
    "else:\n",
    "    print(\"The most observed emotion: Neutral\")\n",
    "\n",
    "\n",
    "# Print the durations and most observed features for eyes\n",
    "print(f\"\\nEye Movements:\")\n",
    "print(f\"Duration taken looking right: {duration_looking_right} sec\")\n",
    "print(f\"Duration taken closed eyes: {duration_eyes_closed} sec\")\n",
    "print(f\"Duration taken looking left: {duration_looking_left} sec\")\n",
    "print(f\"Duration taken looking straight: {duration_looking_straight} sec\")\n",
    "\n",
    "# Determine the most observed eye movement\n",
    "max_eye_duration = max(duration_looking_right, duration_eyes_closed, duration_looking_left, duration_looking_straight)\n",
    "if max_eye_duration == duration_looking_right:\n",
    "    print(\"The most observed eye movement: Looking Right\")\n",
    "elif max_eye_duration == duration_eyes_closed:\n",
    "    print(\"The most observed eye movement: Eyes Closed\")\n",
    "elif max_eye_duration == duration_looking_left:\n",
    "    print(\"The most observed eye movement: Looking Left\")\n",
    "else:\n",
    "    print(\"The most observed eye movement: Looking Straight\")\n",
    "\n",
    "# Print the durations and most observed features for head pose\n",
    "print(f\"\\nHead Pose Estimation:\")\n",
    "print(f\"Duration of Time Looking Forward: {time_forward_seconds} seconds\")\n",
    "print(f\"Duration of Time Looking Up: {time_up_seconds} seconds\")\n",
    "print(f\"Duration of Time Looking Left: q{time_left_seconds} seconds\")\n",
    "print(f\"Duration of Time Looking Right: {time_right_seconds} seconds\")\n",
    "print(f\"Duration of Time Looking Down: {time_down_seconds} seconds\")\n",
    "\n",
    "# Determine the most observed eye movement\n",
    "max_eye_duration = max(time_forward_seconds, time_up_seconds, time_left_seconds, time_right_seconds, time_down_seconds)\n",
    "if max_eye_duration == time_forward_seconds:\n",
    "    print('\\033[93m'+\"The most observed head pose: Facing Forward\")\n",
    "elif max_eye_duration == time_up_seconds:\n",
    "    print('\\033[93m'+\"The most observed head pose: Facing Upwards\")\n",
    "elif max_eye_duration == time_left_seconds:\n",
    "    print('\\033[93m'+\"The most observed head pose: Facing Left\")\n",
    "elif max_eye_duration == time_right_seconds:\n",
    "    print('\\033[93m'+\"The most observed head pose: Facing Right\")\n",
    "else:\n",
    "    print('\\033[93m'+\"The most observed head pose: Facing Downwards\")\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373094c8-0a29-4d05-b2af-000739669547",
   "metadata": {},
   "source": [
    "!pip install cv2\n",
    "!pip install dlib\n",
    "!pip install numpy\n",
    "!pip install ipython\n",
    "!pip install scipy\n",
    "!pip install imutils\n",
    "!pip install tensorflow\n",
    "!pip install mediapipe\n",
    "\n",
    "import cv2\n",
    "import dlib\n",
    "import numpy as np\n",
    "from IPython.display import display, clear_output\n",
    "from scipy.spatial import distance\n",
    "from imutils import face_utils\n",
    "from scipy.ndimage import zoom\n",
    "from tensorflow.keras.models import load_model\n",
    "import mediapipe as mp\n",
    "import time\n",
    "import csv\n",
    "\n",
    "# Function to detect eyes in a frame\n",
    "def detect_eyes(frame, shape):\n",
    "    left_eye = shape[36:42]\n",
    "    right_eye = shape[42:48]\n",
    "    return left_eye, right_eye\n",
    "\n",
    "# Function to calculate Eye Aspect Ratio (EAR)\n",
    "def calculate_ear(eye):\n",
    "    eye = np.array([(point[0], point[1]) for point in eye])\n",
    "    A = np.linalg.norm(eye[1] - eye[5])\n",
    "    B = np.linalg.norm(eye[2] - eye[4])\n",
    "    C = np.linalg.norm(eye[0] - eye[3])\n",
    "    ear = (A + B) / (2.0 * C)\n",
    "    return ear\n",
    "\n",
    "# Load dlib face detector and facial landmarks predictor\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"C:/Users/nisha/Downloads/brainy-bits/brainy-bits-ai/notebooks/shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "# Load emotion detection model\n",
    "emotion_model = load_model(\"C:/Users/nisha/Downloads/brainy-bits/brainy-bits-ai/notebooks/video.h5\")\n",
    "\n",
    "# Initialize video capture from the camera\n",
    "cap = cv2.VideoCapture(0)  # 0 corresponds to the default camera (you can change it if you have multiple cameras)\n",
    "\n",
    "# Get video properties for the output video\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# Initialize video writer for the output video\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "output_video = cv2.VideoWriter('/content/outputvideo.avi', fourcc, fps, (width, height))\n",
    "\n",
    "# Initialize variables to record durations\n",
    "duration_eyes_closed = {}\n",
    "duration_looking_left = {}\n",
    "duration_looking_right = {}\n",
    "duration_looking_straight = {}\n",
    "\n",
    "# Initialize variables for counting eye movement\n",
    "count_left = {}\n",
    "count_right = {}\n",
    "count_straight = {}\n",
    "\n",
    "# Variables to track emotion detected\n",
    "emotion_start_time = time.time()\n",
    "emotion_duration = {\"angry\": {}, \"sad\": {}, \"happy\": {}, \"fear\": {}, \"disgust\": {}, \"neutral\": {}, \"surprise\": {}}\n",
    "\n",
    "# Variables to track time spent in different head pose directions\n",
    "time_forward_seconds = {}\n",
    "time_left_seconds = {}\n",
    "time_right_seconds = {}\n",
    "time_up_seconds = {}\n",
    "time_down_seconds = {}\n",
    "\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = detector(gray)\n",
    "\n",
    "    for i, face in enumerate(faces):\n",
    "        shape = predictor(gray, face)\n",
    "        shape = face_utils.shape_to_np(shape)\n",
    "\n",
    "        person_id = f\"Person {i+1}\"\n",
    "\n",
    "        if person_id not in duration_eyes_closed:\n",
    "            duration_eyes_closed[person_id] = 0\n",
    "            duration_looking_left[person_id] = 0\n",
    "            duration_looking_right[person_id] = 0\n",
    "            duration_looking_straight[person_id] = 0\n",
    "            count_left[person_id] = 0\n",
    "            count_right[person_id] = 0\n",
    "            count_straight[person_id] = 0\n",
    "            time_forward_seconds[person_id] = 0\n",
    "            time_left_seconds[person_id] = 0\n",
    "            time_right_seconds[person_id] = 0\n",
    "            time_up_seconds[person_id] = 0\n",
    "            time_down_seconds[person_id] = 0\n",
    "            for emotion in emotion_duration:\n",
    "                emotion_duration[emotion][person_id] = 0\n",
    "\n",
    "        # Eye tracking\n",
    "        left_eye, right_eye = detect_eyes(frame, shape)\n",
    "\n",
    "        if left_eye is not None and right_eye is not None:\n",
    "            ear_left = calculate_ear(left_eye)\n",
    "            ear_right = calculate_ear(right_eye)\n",
    "\n",
    "            # Calculate the average EAR for both eyes\n",
    "            avg_ear = (ear_left + ear_right) / 2.0\n",
    "\n",
    "            # Set a threshold for distraction detection (you may need to adjust this)\n",
    "            distraction_threshold = 0.2\n",
    "\n",
    "            # Check if the person is distracted\n",
    "            if avg_ear < distraction_threshold:\n",
    "                cv2.putText(frame, f\"{person_id}: Eyes Closed\", (10, 30 + i*30), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 0, 255), 2)\n",
    "                duration_eyes_closed[person_id] += 1 / fps  # Increment the duration\n",
    "                count_straight[person_id] += 1\n",
    "\n",
    "            else:\n",
    "                # Check gaze direction\n",
    "                horizontal_ratio = (left_eye[0][0] + right_eye[3][0]) / 2 / width\n",
    "                if horizontal_ratio < 0.4:\n",
    "                    cv2.putText(frame, f\"{person_id}: Looking Left\", (10, 30 + i*30), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 0), 2)\n",
    "                    duration_looking_left[person_id] += 1 / fps  # Increment the duration\n",
    "                    count_left[person_id] += 1\n",
    "                elif horizontal_ratio > 0.6:\n",
    "                    cv2.putText(frame, f\"{person_id}: Looking Right\", (10, 30 + i*30), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 0), 2)\n",
    "                    duration_looking_right[person_id] += 1 / fps  # Increment the duration\n",
    "                    count_right[person_id] += 1\n",
    "                else:\n",
    "                    cv2.putText(frame, f\"{person_id}: Looking Straight\", (10, 30 + i*30), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 0), 2)\n",
    "                    duration_looking_straight[person_id] += 1 / fps  # Increment the duration\n",
    "\n",
    "            # Draw contours around eyes\n",
    "            for eye in [left_eye, right_eye]:\n",
    "                for point in eye:\n",
    "                    x, y = point[0], point[1]\n",
    "                    cv2.circle(frame, (x, y), 3, (0, 255, 0), -1)\n",
    "\n",
    "        # Emotion detection\n",
    "        (x, y, w, h) = face_utils.rect_to_bb(face)\n",
    "        face_crop = gray[y:y + h, x:x + w]\n",
    "        face_crop = zoom(face_crop, (48 / face_crop.shape[0], 48 / face_crop.shape[1]))\n",
    "        face_crop = face_crop.astype(np.float32)\n",
    "        face_crop /= float(face_crop.max())\n",
    "        face_crop = np.reshape(face_crop.flatten(), (1, 48, 48, 1))\n",
    "\n",
    "        prediction = emotion_model.predict(face_crop)\n",
    "        prediction_result = np.argmax(prediction)\n",
    "\n",
    "        # Rectangle around the face\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "        # Annotate main image with emotion label\n",
    "        emotion_labels = [\"Angry\", \"Disgust\", \"Fear\", \"Happy\", \"Sad\", \"Surprise\", \"Neutral\"]\n",
    "        emotion_label = emotion_labels[prediction_result]\n",
    "        cv2.putText(frame, f\"{person_id}: {emotion_label}\", (x + w - 10, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "        emotion_duration[emotion_label.lower()][person_id] += time.time() - emotion_start_time\n",
    "        emotion_start_time = time.time()\n",
    "\n",
    "        # Head pose estimation\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame_rgb.flags.writeable = False\n",
    "        results = face_mesh.process(frame_rgb)\n",
    "        frame_rgb.flags.writeable = True\n",
    "        frame = cv2.cvtColor(frame_rgb, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        img_h, img_w, img_c = frame.shape\n",
    "        face_3d = []\n",
    "        face_2d = []\n",
    "\n",
    "        if results.multi_face_landmarks:\n",
    "            for face_landmarks in results.multi_face_landmarks:\n",
    "                for idx, lm in enumerate(face_landmarks.landmark):\n",
    "                    if idx == 33 or idx == 263 or idx == 1 or idx == 61 or idx == 291 or idx == 199:\n",
    "                        if idx == 1:\n",
    "                            nose_2d = (lm.x * img_w, lm.y * img_h)\n",
    "                            nose_3d = (lm.x * img_w, lm.y * img_h, lm.z * 8000)\n",
    "\n",
    "                        x, y = int(lm.x * img_w), int(lm.y * img_h)\n",
    "\n",
    "                        # Get the 2D Coordinates\n",
    "                        face_2d.append([x, y])\n",
    "\n",
    "                        # Get\n",
    "                        # Get the 3D Coordinates\n",
    "                        face_3d.append([x, y, lm.z])\n",
    "\n",
    "                face_2d = np.array(face_2d, dtype=np.float64)\n",
    "                face_3d = np.array(face_3d, dtype=np.float64)\n",
    "\n",
    "                # Camera matrix\n",
    "                focal_length = 1 * img_w\n",
    "                cam_matrix = np.array([[focal_length, 0, img_w / 2],\n",
    "                                       [0, focal_length, img_h / 2],\n",
    "                                       [0, 0, 1]])\n",
    "\n",
    "                # Distortion parameters\n",
    "                dist_matrix = np.zeros((4, 1), dtype=np.float64)\n",
    "\n",
    "                # Solve PnP\n",
    "                success, rot_vec, trans_vec = cv2.solvePnP(face_3d, face_2d, cam_matrix, dist_matrix)\n",
    "\n",
    "                # Get rotational matrix\n",
    "                rmat, jac = cv2.Rodrigues(rot_vec)\n",
    "\n",
    "                # Get angles\n",
    "                angles, mtx_r, mtx_q, qx, qy, qz = cv2.RQDecomp3x3(rmat)\n",
    "\n",
    "                # Get the y rotation degree\n",
    "                x_angle = angles[0] * 360\n",
    "                y_angle = angles[1] * 360\n",
    "                z_angle = angles[2] * 360\n",
    "\n",
    "                # See where the user's head tilting\n",
    "                if y_angle < -10:\n",
    "                    text = \"Looking Left\"\n",
    "                    time_left_seconds[person_id] += 1 / fps\n",
    "                elif y_angle > 10:\n",
    "                    text = \"Looking Right\"\n",
    "                    time_right_seconds[person_id] += 1 / fps\n",
    "                elif x_angle < -10:\n",
    "                    text = \"Looking Down\"\n",
    "                    time_down_seconds[person_id] += 1 / fps\n",
    "                elif x_angle > 10:\n",
    "                    text = \"Looking Up\"\n",
    "                    time_up_seconds[person_id] += 1 / fps\n",
    "                else:\n",
    "                    text = \"Looking Forward\"\n",
    "                    time_forward_seconds[person_id] += 1 / fps\n",
    "\n",
    "                # Display the text\n",
    "                cv2.putText(frame, f\"{person_id}: {text}\", (500, 50 + i * 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    # Write the frame to the output video\n",
    "    output_video.write(frame)\n",
    "\n",
    "    # Display the frame in a window\n",
    "    cv2.imshow('Frame', frame)\n",
    "    \n",
    "    # Clear the output in Jupyter notebook\n",
    "    clear_output(wait=True)\n",
    "    display(frame)\n",
    "\n",
    "    # Exit the loop when the 'q' key is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture and writer objects\n",
    "cap.release()\n",
    "output_video.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Write the recorded data to a CSV file\n",
    "with open('eye_tracking_data.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Person ID\", \"Duration Eyes Closed (s)\", \"Duration Looking Left (s)\", \"Duration Looking Right (s)\", \"Duration Looking Straight (s)\", \"Left Counts\", \"Right Counts\", \"Straight Counts\"])\n",
    "    for person_id in duration_eyes_closed:\n",
    "        writer.writerow([person_id, duration_eyes_closed[person_id], duration_looking_left[person_id], duration_looking_right[person_id], duration_looking_straight[person_id], count_left[person_id], count_right[person_id], count_straight[person_id]])\n",
    "\n",
    "with open('emotion_detection_data.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Person ID\", \"Angry (s)\", \"Sad (s)\", \"Happy (s)\", \"Fear (s)\", \"Disgust (s)\", \"Neutral (s)\", \"Surprise (s)\"])\n",
    "    for person_id in emotion_duration[\"angry\"]:\n",
    "        writer.writerow([person_id, emotion_duration[\"angry\"][person_id], emotion_duration[\"sad\"][person_id], emotion_duration[\"happy\"][person_id], emotion_duration[\"fear\"][person_id], emotion_duration[\"disgust\"][person_id], emotion_duration[\"neutral\"][person_id], emotion_duration[\"surprise\"][person_id]])\n",
    "\n",
    "with open('head_pose_data.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Person ID\", \"Looking Forward (s)\", \"Looking Left (s)\", \"Looking Right (s)\", \"Looking Up (s)\", \"Looking Down (s)\"])\n",
    "    for person_id in time_forward_seconds:\n",
    "        writer.writerow([person_id, time_forward_seconds[person_id], time_left_seconds[person_id], time_right_seconds[person_id], time_up_seconds[person_id], time_down_seconds[person_id]])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f3ea9e-719e-4e12-958f-5cbd5b5a7719",
   "metadata": {},
   "source": [
    "pip install --user opencv-contrib-python ml-dtypes sounddevice jaxlib jax mediapipe"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f49d73-37bd-4059-9e90-594c63d2b24f",
   "metadata": {},
   "source": [
    "import cv2\n",
    "import dlib\n",
    "import numpy as np\n",
    "from IPython.display import display, clear_output\n",
    "from scipy.spatial import distance\n",
    "from imutils import face_utils\n",
    "from scipy.ndimage import zoom\n",
    "from tensorflow.keras.models import load_model\n",
    "import mediapipe as mp\n",
    "import time\n",
    "import csv\n",
    "\n",
    "# Function to detect eyes in a frame\n",
    "def detect_eyes(frame, shape):\n",
    "    left_eye = shape[36:42]\n",
    "    right_eye = shape[42:48]\n",
    "    return left_eye, right_eye\n",
    "\n",
    "# Function to calculate Eye Aspect Ratio (EAR)\n",
    "def calculate_ear(eye):\n",
    "    eye = np.array([(point[0], point[1]) for point in eye])\n",
    "    A = np.linalg.norm(eye[1] - eye[5])\n",
    "    B = np.linalg.norm(eye[2] - eye[4])\n",
    "    C = np.linalg.norm(eye[0] - eye[3])\n",
    "    ear = (A + B) / (2.0 * C)\n",
    "    return ear\n",
    "\n",
    "# Load dlib face detector and facial landmarks predictor\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"C:/Users/nisha/Downloads/brainy-bits/brainy-bits-ai/notebooks/shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "# Load emotion detection model\n",
    "emotion_model = load_model(\"C:/Users/nisha/Downloads/brainy-bits/brainy-bits-ai/notebooks/video.h5\")\n",
    "\n",
    "# Initialize video capture from the camera\n",
    "cap = cv2.VideoCapture(0)  # 0 corresponds to the default camera (you can change it if you have multiple cameras)\n",
    "\n",
    "# Get video properties for the output video\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# Initialize video writer for the output video\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "output_video = cv2.VideoWriter('/content/outputvideo.avi', fourcc, fps, (width, height))\n",
    "\n",
    "# Initialize variables to record durations\n",
    "duration_eyes_closed = {}\n",
    "duration_looking_left = {}\n",
    "duration_looking_right = {}\n",
    "duration_looking_straight = {}\n",
    "\n",
    "# Initialize variables for counting eye movement\n",
    "count_left = {}\n",
    "count_right = {}\n",
    "count_straight = {}\n",
    "\n",
    "# Variables to track emotion detected\n",
    "emotion_start_time = time.time()\n",
    "emotion_duration = {\"angry\": {}, \"sad\": {}, \"happy\": {}, \"fear\": {}, \"disgust\": {}, \"neutral\": {}, \"surprise\": {}}\n",
    "\n",
    "# Variables to track time spent in different head pose directions\n",
    "time_forward_seconds = {}\n",
    "time_left_seconds = {}\n",
    "time_right_seconds = {}\n",
    "time_up_seconds = {}\n",
    "time_down_seconds = {}\n",
    "\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = detector(gray)\n",
    "\n",
    "    for i, face in enumerate(faces):\n",
    "        shape = predictor(gray, face)\n",
    "        shape = face_utils.shape_to_np(shape)\n",
    "\n",
    "        person_id = f\"Person {i+1}\"\n",
    "\n",
    "        if person_id not in duration_eyes_closed:\n",
    "            duration_eyes_closed[person_id] = 0\n",
    "            duration_looking_left[person_id] = 0\n",
    "            duration_looking_right[person_id] = 0\n",
    "            duration_looking_straight[person_id] = 0\n",
    "            count_left[person_id] = 0\n",
    "            count_right[person_id] = 0\n",
    "            count_straight[person_id] = 0\n",
    "            time_forward_seconds[person_id] = 0\n",
    "            time_left_seconds[person_id] = 0\n",
    "            time_right_seconds[person_id] = 0\n",
    "            time_up_seconds[person_id] = 0\n",
    "            time_down_seconds[person_id] = 0\n",
    "            for emotion in emotion_duration:\n",
    "                emotion_duration[emotion][person_id] = 0\n",
    "\n",
    "        # Eye tracking\n",
    "        left_eye, right_eye = detect_eyes(frame, shape)\n",
    "\n",
    "        if left_eye is not None and right_eye is not None:\n",
    "            ear_left = calculate_ear(left_eye)\n",
    "            ear_right = calculate_ear(right_eye)\n",
    "\n",
    "            # Calculate the average EAR for both eyes\n",
    "            avg_ear = (ear_left + ear_right) / 2.0\n",
    "\n",
    "            # Set a threshold for distraction detection (you may need to adjust this)\n",
    "            distraction_threshold = 0.2\n",
    "\n",
    "            # Check if the person is distracted\n",
    "            if avg_ear < distraction_threshold:\n",
    "                cv2.putText(frame, f\"{person_id}: Eyes Closed\", (10, 30 + i*30), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 0, 255), 2)\n",
    "                duration_eyes_closed[person_id] += 1 / fps  # Increment the duration\n",
    "                count_straight[person_id] += 1\n",
    "\n",
    "            else:\n",
    "                # Check gaze direction\n",
    "                horizontal_ratio = (left_eye[0][0] + right_eye[3][0]) / 2 / width\n",
    "                if horizontal_ratio < 0.4:\n",
    "                    cv2.putText(frame, f\"{person_id}: Looking Left\", (10, 30 + i*30), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 0), 2)\n",
    "                    duration_looking_left[person_id] += 1 / fps  # Increment the duration\n",
    "                    count_left[person_id] += 1\n",
    "                elif horizontal_ratio > 0.6:\n",
    "                    cv2.putText(frame, f\"{person_id}: Looking Right\", (10, 30 + i*30), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 0), 2)\n",
    "                    duration_looking_right[person_id] += 1 / fps  # Increment the duration\n",
    "                    count_right[person_id] += 1\n",
    "                else:\n",
    "                    cv2.putText(frame, f\"{person_id}: Looking Straight\", (10, 30 + i*30), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 0), 2)\n",
    "                    duration_looking_straight[person_id] += 1 / fps  # Increment the duration\n",
    "\n",
    "            # Draw contours around eyes\n",
    "            for eye in [left_eye, right_eye]:\n",
    "                for point in eye:\n",
    "                    x, y = point[0], point[1]\n",
    "                    cv2.circle(frame, (x, y), 3, (0, 255, 0), -1)\n",
    "\n",
    "        # Emotion detection\n",
    "        (x, y, w, h) = face_utils.rect_to_bb(face)\n",
    "        if w > 0 and h > 0:  # Check if width and height are positive\n",
    "            face_crop = gray[y:y + h, x:x + w]\n",
    "            if face_crop.shape[0] > 0 and face_crop.shape[1] > 0:  # Check if face crop dimensions are positive\n",
    "                face_crop = zoom(face_crop, (48 / face_crop.shape[0], 48 / face_crop.shape[1]))\n",
    "                face_crop = face_crop.astype(np.float32)\n",
    "                face_crop /= float(face_crop.max())\n",
    "                face_crop = np.reshape(face_crop.flatten(), (1, 48, 48, 1))\n",
    "\n",
    "                prediction = emotion_model.predict(face_crop)\n",
    "                prediction_result = np.argmax(prediction)\n",
    "\n",
    "                # Rectangle around the face\n",
    "                cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "                # Annotate main image with emotion label\n",
    "                emotion_labels = [\"Angry\", \"Disgust\", \"Fear\", \"Happy\", \"Sad\", \"Surprise\", \"Neutral\"]\n",
    "                emotion_label = emotion_labels[prediction_result]\n",
    "                cv2.putText(frame, f\"{person_id}: {emotion_label}\", (x + w - 10, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "                emotion_duration[emotion_label.lower()][person_id] += time.time() - emotion_start_time\n",
    "                emotion_start_time = time.time()\n",
    "\n",
    "        # Head pose estimation\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame_rgb.flags.writeable = False\n",
    "        results = face_mesh.process(frame_rgb)\n",
    "        frame_rgb.flags.writeable = True\n",
    "        frame = cv2.cvtColor(frame_rgb, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        img_h, img_w, img_c = frame.shape\n",
    "        face_3d = []\n",
    "        face_2d = []\n",
    "\n",
    "        if results.multi_face_landmarks:\n",
    "            for face_landmarks in results.multi_face_landmarks:\n",
    "                for idx, lm in enumerate(face_landmarks.landmark):\n",
    "                    if idx == 33 or idx == 263 or idx == 1 or idx == 61 or idx == 291 or idx == 199:\n",
    "                        nose_2d = (lm.x * img_w, lm.y * img_h)\n",
    "                        nose_3d = (lm.x * img_w, lm.y * img_h, lm.z * 8000)\n",
    "\n",
    "                    x, y = int(lm.x * img_w), int(lm.y * img_h)\n",
    "\n",
    "                    # Get the 2D Coordinates\n",
    "                    face_2d.append([x, y])\n",
    "\n",
    "                    # Get the 3D Coordinates\n",
    "                    face_3d.append([x, y, lm.z])\n",
    "\n",
    "                face_2d = np.array(face_2d, dtype=np.float64)\n",
    "                face_3d = np.array(face_3d, dtype=np.float64)\n",
    "\n",
    "                # Camera matrix\n",
    "                focal_length = 1 * img_w\n",
    "                cam_matrix = np.array([[focal_length, 0, img_w / 2],\n",
    "                                       [0, focal_length, img_h / 2],\n",
    "                                       [0, 0, 1]])\n",
    "\n",
    "                # Distortion parameters\n",
    "                dist_matrix = np.zeros((4, 1), dtype=np.float64)\n",
    "\n",
    "                # Solve PnP\n",
    "                success, rot_vec, trans_vec = cv2.solvePnP(face_3d, face_2d, cam_matrix, dist_matrix)\n",
    "\n",
    "                # Get rotational matrix\n",
    "                rmat, jac = cv2.Rodrigues(rot_vec)\n",
    "\n",
    "                # Get angles\n",
    "                angles, mtx_r, mtx_q, qx, qy, qz = cv2.RQDecomp3x3(rmat)\n",
    "\n",
    "                # Get the y rotation degree\n",
    "                x_angle = angles[0] * 360\n",
    "                y_angle = angles[1] * 360\n",
    "                z_angle = angles[2] * 360\n",
    "\n",
    "                # See where the user's head tilting\n",
    "                if y_angle < -10:\n",
    "                    text = \"Looking Left\"\n",
    "                    time_left_seconds[person_id] += 1 / fps\n",
    "                elif y_angle > 10:\n",
    "                    text = \"Looking Right\"\n",
    "                    time_right_seconds[person_id] += 1 / fps\n",
    "                elif x_angle < -10:\n",
    "                    text = \"Looking Down\"\n",
    "                    time_down_seconds[person_id] += 1 / fps\n",
    "                elif x_angle > 10:\n",
    "                    text = \"Looking Up\"\n",
    "                    time_up_seconds[person_id] += 1 / fps\n",
    "                else:\n",
    "                    text = \"Looking Forward\"\n",
    "                    time_forward_seconds[person_id] += 1 / fps\n",
    "\n",
    "                # Display the text\n",
    "                cv2.putText(frame, f\"{person_id}: {text}\", (500, 50 + i * 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    # Write the frame to the output video\n",
    "    output_video.write(frame)\n",
    "\n",
    "    # Display the frame in a window\n",
    "    cv2.imshow('Frame', frame)\n",
    "    \n",
    "    # Clear the output in Jupyter notebook\n",
    "    clear_output(wait=True)\n",
    "    display(frame)\n",
    "\n",
    "    # Exit the loop when the 'q' key is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture and writer objects\n",
    "cap.release()\n",
    "output_video.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Write the recorded data to a CSV file\n",
    "with open('eye_tracking_data.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Person ID\", \"Duration Eyes Closed (s)\", \"Duration Looking Left (s)\", \"Duration Looking Right (s)\", \"Duration Looking Straight (s)\", \"Left Counts\", \"Right Counts\", \"Straight Counts\"])\n",
    "    for person_id in duration_eyes_closed:\n",
    "        writer.writerow([person_id, duration_eyes_closed[person_id], duration_looking_left[person_id], duration_looking_right[person_id], duration_looking_straight[person_id], count_left[person_id], count_right[person_id], count_straight[person_id]])\n",
    "\n",
    "with open('emotion_detection_data.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Person ID\", \"Angry (s)\", \"Sad (s)\", \"Happy (s)\", \"Fear (s)\", \"Disgust (s)\", \"Neutral (s)\", \"Surprise (s)\"])\n",
    "    for person_id in emotion_duration[\"angry\"]:\n",
    "        writer.writerow([person_id, emotion_duration[\"angry\"][person_id], emotion_duration[\"sad\"][person_id], emotion_duration[\"happy\"][person_id], emotion_duration[\"fear\"][person_id], emotion_duration[\"disgust\"][person_id], emotion_duration[\"neutral\"][person_id], emotion_duration[\"surprise\"][person_id]])\n",
    "\n",
    "with open('head_pose_data.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Person ID\", \"Looking Forward (s)\", \"Looking Left (s)\", \"Looking Right (s)\", \"Looking Up (s)\", \"Looking Down (s)\"])\n",
    "    for person_id in time_forward_seconds:\n",
    "        writer.writerow([person_id, time_forward_seconds[person_id], time_left_seconds[person_id], time_right_seconds[person_id], time_up_seconds[person_id], time_down_seconds[person_id]])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f4c9f5-0657-4b93-a2dc-89240ebd3997",
   "metadata": {},
   "source": [
    "import cv2\n",
    "import dlib\n",
    "import numpy as np\n",
    "from IPython.display import display, clear_output\n",
    "from scipy.spatial import distance\n",
    "from imutils import face_utils\n",
    "from scipy.ndimage import zoom\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "import mediapipe as mp\n",
    "import time\n",
    "import csv\n",
    "\n",
    "# Custom deserialization function for VarianceScaling initializer\n",
    "def custom_initializer(config):\n",
    "    if 'dtype' in config:\n",
    "        del config['dtype']\n",
    "    return tf.keras.initializers.VarianceScaling(**config)\n",
    "\n",
    "# Register the custom deserialization function\n",
    "tf.keras.utils.get_custom_objects().update({\n",
    "    'VarianceScaling': custom_initializer\n",
    "})\n",
    "\n",
    "# Function to detect eyes in a frame\n",
    "def detect_eyes(frame, shape):\n",
    "    left_eye = shape[36:42]\n",
    "    right_eye = shape[42:48]\n",
    "    return left_eye, right_eye\n",
    "\n",
    "# Function to calculate Eye Aspect Ratio (EAR)\n",
    "def calculate_ear(eye):\n",
    "    eye = np.array([(point[0], point[1]) for point in eye])\n",
    "    A = np.linalg.norm(eye[1] - eye[5])\n",
    "    B = np.linalg.norm(eye[2] - eye[4])\n",
    "    C = np.linalg.norm(eye[0] - eye[3])\n",
    "    ear = (A + B) / (2.0 * C)\n",
    "    return ear\n",
    "\n",
    "# Load dlib face detector and facial landmarks predictor\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"brainy-bits/brainy-bits-ai/notebooks/shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "# Load emotion detection model\n",
    "emotion_model = load_model(\"brainy-bits/brainy-bits-ai/notebooks/video.h5\")\n",
    "\n",
    "# Initialize video capture from the camera\n",
    "cap = cv2.VideoCapture(0)  # 0 corresponds to the default camera (you can change it if you have multiple cameras)\n",
    "\n",
    "# Get video properties for the output video\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# Initialize video writer for the output video\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "output_video = cv2.VideoWriter('/content/outputvideo.avi', fourcc, fps, (width, height))\n",
    "\n",
    "# Initialize variables to record durations\n",
    "duration_eyes_closed = {}\n",
    "duration_looking_left = {}\n",
    "duration_looking_right = {}\n",
    "duration_looking_straight = {}\n",
    "\n",
    "# Initialize variables for counting eye movement\n",
    "count_left = {}\n",
    "count_right = {}\n",
    "count_straight = {}\n",
    "\n",
    "# Variables to track emotion detected\n",
    "emotion_start_time = time.time()\n",
    "emotion_duration = {\"angry\": {}, \"sad\": {}, \"happy\": {}, \"fear\": {}, \"disgust\": {}, \"neutral\": {}, \"surprise\": {}}\n",
    "\n",
    "# Variables to track time spent in different head pose directions\n",
    "time_forward_seconds = {}\n",
    "time_left_seconds = {}\n",
    "time_right_seconds = {}\n",
    "time_up_seconds = {}\n",
    "time_down_seconds = {}\n",
    "\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = detector(gray)\n",
    "\n",
    "    for i, face in enumerate(faces):\n",
    "        shape = predictor(gray, face)\n",
    "        shape = face_utils.shape_to_np(shape)\n",
    "\n",
    "        person_id = f\"Person {i+1}\"\n",
    "\n",
    "        if person_id not in duration_eyes_closed:\n",
    "            duration_eyes_closed[person_id] = 0\n",
    "            duration_looking_left[person_id] = 0\n",
    "            duration_looking_right[person_id] = 0\n",
    "            duration_looking_straight[person_id] = 0\n",
    "            count_left[person_id] = 0\n",
    "            count_right[person_id] = 0\n",
    "            count_straight[person_id] = 0\n",
    "            time_forward_seconds[person_id] = 0\n",
    "            time_left_seconds[person_id] = 0\n",
    "            time_right_seconds[person_id] = 0\n",
    "            time_up_seconds[person_id] = 0\n",
    "            time_down_seconds[person_id] = 0\n",
    "            for emotion in emotion_duration:\n",
    "                emotion_duration[emotion][person_id] = 0\n",
    "\n",
    "        # Eye tracking\n",
    "        left_eye, right_eye = detect_eyes(frame, shape)\n",
    "\n",
    "        if left_eye is not None and right_eye is not None:\n",
    "            ear_left = calculate_ear(left_eye)\n",
    "            ear_right = calculate_ear(right_eye)\n",
    "\n",
    "            # Calculate the average EAR for both eyes\n",
    "            avg_ear = (ear_left + ear_right) / 2.0\n",
    "\n",
    "            # Set a threshold for distraction detection (you may need to adjust this)\n",
    "            distraction_threshold = 0.2\n",
    "\n",
    "            # Check if the person is distracted\n",
    "            if avg_ear < distraction_threshold:\n",
    "                cv2.putText(frame, f\"{person_id}: Eyes Closed\", (10, 30 + i*30), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 0, 255), 2)\n",
    "                duration_eyes_closed[person_id] += 1 / fps  # Increment the duration\n",
    "                count_straight[person_id] += 1\n",
    "\n",
    "            else:\n",
    "                # Check gaze direction\n",
    "                horizontal_ratio = (left_eye[0][0] + right_eye[3][0]) / 2 / width\n",
    "                if horizontal_ratio < 0.4:\n",
    "                    cv2.putText(frame, f\"{person_id}: Looking Left\", (10, 30 + i*30), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 0), 2)\n",
    "                    duration_looking_left[person_id] += 1 / fps  # Increment the duration\n",
    "                    count_left[person_id] += 1\n",
    "                elif horizontal_ratio > 0.6:\n",
    "                    cv2.putText(frame, f\"{person_id}: Looking Right\", (10, 30 + i*30), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 0), 2)\n",
    "                    duration_looking_right[person_id] += 1 / fps  # Increment the duration\n",
    "                    count_right[person_id] += 1\n",
    "                else:\n",
    "                    cv2.putText(frame, f\"{person_id}: Looking Straight\", (10, 30 + i*30), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 0), 2)\n",
    "                    duration_looking_straight[person_id] += 1 / fps  # Increment the duration\n",
    "\n",
    "            # Draw contours around eyes\n",
    "            for eye in [left_eye, right_eye]:\n",
    "                for point in eye:\n",
    "                    x, y = point[0], point[1]\n",
    "                    cv2.circle(frame, (x, y), 3, (0, 255, 0), -1)\n",
    "\n",
    "        # Emotion detection\n",
    "        (x, y, w, h) = face_utils.rect_to_bb(face)\n",
    "        if w > 0 and h > 0:  # Check if width and height are positive\n",
    "            face_crop = gray[y:y + h, x:x + w]\n",
    "            if face_crop.shape[0] > 0 and face_crop.shape[1] > 0:  # Check if face crop dimensions are positive\n",
    "                face_crop = zoom(face_crop, (48 / face_crop.shape[0], 48 / face_crop.shape[1]))\n",
    "                face_crop = face_crop.astype(np.float32)\n",
    "                face_crop /= float(face_crop.max())\n",
    "                face_crop = np.reshape(face_crop.flatten(), (1, 48, 48, 1))\n",
    "\n",
    "                prediction = emotion_model.predict(face_crop)\n",
    "                prediction_result = np.argmax(prediction)\n",
    "\n",
    "                # Rectangle around the face\n",
    "                cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "                # Annotate main image with emotion label\n",
    "                emotion_labels = [\"Angry\", \"Disgust\", \"Fear\", \"Happy\", \"Sad\", \"Surprise\", \"Neutral\"]\n",
    "                emotion_label = emotion_labels[prediction_result]\n",
    "                cv2.putText(frame, f\"{person_id}: {emotion_label}\", (x + w - 10, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "                emotion_duration[emotion_label.lower()][person_id] += time.time() - emotion_start_time\n",
    "                emotion_start_time = time.time()\n",
    "\n",
    "        # Head pose estimation\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame_rgb.flags.writeable = False\n",
    "        results = face_mesh.process(frame_rgb)\n",
    "        frame_rgb.flags.writeable = True\n",
    "        frame = cv2.cvtColor(frame_rgb, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        img_h, img_w, img_c = frame.shape\n",
    "        face_3d = []\n",
    "        face_2d = []\n",
    "\n",
    "        if results.multi_face_landmarks:\n",
    "            for face_landmarks in results.multi_face_landmarks:\n",
    "                for idx, lm in enumerate(face_landmarks.landmark):\n",
    "                    if idx == 33 or idx == 263 or idx == 1 or idx == 61 or idx == 291 or idx == 199:\n",
    "                        nose_2d = (lm.x * img_w, lm.y * img_h)\n",
    "                        nose_3d = (lm.x * img_w, lm.y * img_h, lm.z * 8000)\n",
    "\n",
    "                    x, y = int(lm.x * img_w), int(lm.y * img_h)\n",
    "\n",
    "                    # Get the 2D Coordinates\n",
    "                    face_2d.append([x, y])\n",
    "\n",
    "                    # Get the 3D Coordinates\n",
    "                    face_3d.append([x, y, lm.z])\n",
    "\n",
    "                face_2d = np.array(face_2d, dtype=np.float64)\n",
    "                face_3d = np.array(face_3d, dtype=np.float64)\n",
    "\n",
    "                # Camera matrix\n",
    "                focal_length = 1 * img_w\n",
    "                cam_matrix = np.array([[focal_length, 0, img_w / 2],\n",
    "                                       [0, focal_length, img_h / 2],\n",
    "                                       [0, 0, 1]])\n",
    "\n",
    "                # Distortion parameters\n",
    "                dist_matrix = np.zeros((4, 1), dtype=np.float64)\n",
    "\n",
    "                # Solve PnP\n",
    "                success, rot_vec, trans_vec = cv2.solvePnP(face_3d, face_2d, cam_matrix, dist_matrix)\n",
    "\n",
    "                # Get rotational matrix\n",
    "                rmat, jac = cv2.Rodrigues(rot_vec)\n",
    "\n",
    "                # Get angles\n",
    "                angles, mtx_r, mtx_q, qx, qy, qz = cv2.RQDecomp3x3(rmat)\n",
    "\n",
    "                # Get the y rotation degree\n",
    "                x_angle = angles[0] * 360\n",
    "                y_angle = angles[1] * 360\n",
    "                z_angle = angles[2] * 360\n",
    "\n",
    "                # See where the user's head tilting\n",
    "                if y_angle < -10:\n",
    "                    text = \"Looking Left\"\n",
    "                    time_left_seconds[person_id] += 1 / fps\n",
    "                elif y_angle > 10:\n",
    "                    text = \"Looking Right\"\n",
    "                    time_right_seconds[person_id] += 1 / fps\n",
    "                elif x_angle < -10:\n",
    "                    text = \"Looking Down\"\n",
    "                    time_down_seconds[person_id] += 1 / fps\n",
    "                elif x_angle > 10:\n",
    "                    text = \"Looking Up\"\n",
    "                    time_up_seconds[person_id] += 1 / fps\n",
    "                else:\n",
    "                    text = \"Looking Forward\"\n",
    "                    time_forward_seconds[person_id] += 1 / fps\n",
    "\n",
    "                # Display the text\n",
    "                cv2.putText(frame, f\"{person_id}: {text}\", (500, 50 + i * 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    # Write the frame to the output video\n",
    "    output_video.write(frame)\n",
    "\n",
    "    # Display the frame in a window\n",
    "    cv2.imshow('Frame', frame)\n",
    "    \n",
    "    # Clear the output in Jupyter notebook\n",
    "    clear_output(wait=True)\n",
    "    display(frame)\n",
    "\n",
    "    # Exit the loop when the 'q' key is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture and writer objects\n",
    "cap.release()\n",
    "output_video.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Write the recorded data to a CSV file\n",
    "with open('eye_tracking_data.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Person ID\", \"Duration Eyes Closed (s)\", \"Duration Looking Left (s)\", \"Duration Looking Right (s)\", \"Duration Looking Straight (s)\", \"Left Counts\", \"Right Counts\", \"Straight Counts\"])\n",
    "    for person_id in duration_eyes_closed:\n",
    "        writer.writerow([person_id, duration_eyes_closed[person_id], duration_looking_left[person_id], duration_looking_right[person_id], duration_looking_straight[person_id], count_left[person_id], count_right[person_id], count_straight[person_id]])\n",
    "\n",
    "with open('emotion_detection_data.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Person ID\", \"Angry (s)\", \"Sad (s)\", \"Happy (s)\", \"Fear (s)\", \"Disgust (s)\", \"Neutral (s)\", \"Surprise (s)\"])\n",
    "    for person_id in emotion_duration[\"angry\"]:\n",
    "        writer.writerow([person_id, emotion_duration[\"angry\"][person_id], emotion_duration[\"sad\"][person_id], emotion_duration[\"happy\"][person_id], emotion_duration[\"fear\"][person_id], emotion_duration[\"disgust\"][person_id], emotion_duration[\"neutral\"][person_id], emotion_duration[\"surprise\"][person_id]])\n",
    "\n",
    "with open('head_pose_data.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Person ID\", \"Looking Forward (s)\", \"Looking Left (s)\", \"Looking Right (s)\", \"Looking Up (s)\", \"Looking Down (s)\"])\n",
    "    for person_id in time_forward_seconds:\n",
    "        writer.writerow([person_id, time_forward_seconds[person_id], time_left_seconds[person_id], time_right_seconds[person_id], time_up_seconds[person_id], time_down_seconds[person_id]])\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2ba5f5e-f9a4-4aa5-b157-57e94b3a49bb",
   "metadata": {},
   "source": [
    "import cv2\n",
    "import dlib\n",
    "import numpy as np\n",
    "from IPython.display import display, clear_output\n",
    "from scipy.spatial import distance\n",
    "from imutils import face_utils\n",
    "from scipy.ndimage import zoom\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "import mediapipe as mp\n",
    "import time\n",
    "import csv\n",
    "\n",
    "# Custom deserialization function for VarianceScaling initializer\n",
    "def custom_initializer(config):\n",
    "    if 'dtype' in config:\n",
    "        del config['dtype']\n",
    "    return tf.keras.initializers.VarianceScaling(**config)\n",
    "\n",
    "# Register the custom deserialization function\n",
    "tf.keras.utils.get_custom_objects().update({\n",
    "    'VarianceScaling': custom_initializer\n",
    "})\n",
    "\n",
    "# Function to detect eyes in a frame\n",
    "def detect_eyes(frame, shape):\n",
    "    left_eye = shape[36:42]\n",
    "    right_eye = shape[42:48]\n",
    "    return left_eye, right_eye\n",
    "\n",
    "# Function to calculate Eye Aspect Ratio (EAR)\n",
    "def calculate_ear(eye):\n",
    "    eye = np.array([(point[0], point[1]) for point in eye])\n",
    "    A = np.linalg.norm(eye[1] - eye[5])\n",
    "    B = np.linalg.norm(eye[2] - eye[4])\n",
    "    C = np.linalg.norm(eye[0] - eye[3])\n",
    "    ear = (A + B) / (2.0 * C)\n",
    "    return ear\n",
    "\n",
    "# Load dlib face detector and facial landmarks predictor\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"C:/Users/nisha/Downloads/brainy-bits/brainy-bits-ai/notebooks/shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "# Load emotion detection model\n",
    "emotion_model = load_model(\"C:/Users/nisha/Downloads/brainy-bits/brainy-bits-ai/notebooks/video.h5\")\n",
    "\n",
    "# Initialize video capture from the camera\n",
    "cap = cv2.VideoCapture(0)  # 0 corresponds to the default camera (you can change it if you have multiple cameras)\n",
    "\n",
    "# Get video properties for the output video\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# Initialize video writer for the output video\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "output_video = cv2.VideoWriter('/content/outputvideo.avi', fourcc, fps, (width, height))\n",
    "\n",
    "# Initialize variables to record durations\n",
    "duration_eyes_closed = {}\n",
    "duration_looking_left = {}\n",
    "duration_looking_right = {}\n",
    "duration_looking_straight = {}\n",
    "\n",
    "# Initialize variables for counting eye movement\n",
    "count_left = {}\n",
    "count_right = {}\n",
    "count_straight = {}\n",
    "\n",
    "# Variables to track emotion detected\n",
    "emotion_start_time = time.time()\n",
    "emotion_duration = {\"angry\": {}, \"sad\": {}, \"happy\": {}, \"fear\": {}, \"disgust\": {}, \"neutral\": {}, \"surprise\": {}}\n",
    "\n",
    "# Variables to track time spent in different head pose directions\n",
    "time_forward_seconds = {}\n",
    "time_left_seconds = {}\n",
    "time_right_seconds = {}\n",
    "time_up_seconds = {}\n",
    "time_down_seconds = {}\n",
    "\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = detector(gray)\n",
    "\n",
    "    for i, face in enumerate(faces):\n",
    "        shape = predictor(gray, face)\n",
    "        shape = face_utils.shape_to_np(shape)\n",
    "\n",
    "        person_id = f\"Person {i+1}\"\n",
    "\n",
    "        if person_id not in duration_eyes_closed:\n",
    "            duration_eyes_closed[person_id] = 0\n",
    "            duration_looking_left[person_id] = 0\n",
    "            duration_looking_right[person_id] = 0\n",
    "            duration_looking_straight[person_id] = 0\n",
    "            count_left[person_id] = 0\n",
    "            count_right[person_id] = 0\n",
    "            count_straight[person_id] = 0\n",
    "            time_forward_seconds[person_id] = 0\n",
    "            time_left_seconds[person_id] = 0\n",
    "            time_right_seconds[person_id] = 0\n",
    "            time_up_seconds[person_id] = 0\n",
    "            time_down_seconds[person_id] = 0\n",
    "            for emotion in emotion_duration:\n",
    "                emotion_duration[emotion][person_id] = 0\n",
    "\n",
    "        # Eye tracking\n",
    "        left_eye, right_eye = detect_eyes(frame, shape)\n",
    "\n",
    "        if left_eye is not None and right_eye is not None:\n",
    "            ear_left = calculate_ear(left_eye)\n",
    "            ear_right = calculate_ear(right_eye)\n",
    "\n",
    "            # Calculate the average EAR for both eyes\n",
    "            avg_ear = (ear_left + ear_right) / 2.0\n",
    "\n",
    "            # Set a threshold for distraction detection (you may need to adjust this)\n",
    "            distraction_threshold = 0.2\n",
    "\n",
    "            # Check if the person is distracted\n",
    "            if avg_ear < distraction_threshold:\n",
    "                cv2.putText(frame, f\"{person_id}: Eyes Closed\", (10, 30 + i*30), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 0, 255), 2)\n",
    "                duration_eyes_closed[person_id] += 1 / fps  # Increment the duration\n",
    "                count_straight[person_id] += 1\n",
    "\n",
    "            else:\n",
    "                # Check gaze direction\n",
    "                horizontal_ratio = (left_eye[0][0] + right_eye[3][0]) / 2 / width\n",
    "                if horizontal_ratio < 0.4:\n",
    "                    cv2.putText(frame, f\"{person_id}: Looking Left\", (10, 30 + i*30), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 0), 2)\n",
    "                    duration_looking_left[person_id] += 1 / fps  # Increment the duration\n",
    "                    count_left[person_id] += 1\n",
    "                elif horizontal_ratio > 0.6:\n",
    "                    cv2.putText(frame, f\"{person_id}: Looking Right\", (10, 30 + i*30), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 0), 2)\n",
    "                    duration_looking_right[person_id] += 1 / fps  # Increment the duration\n",
    "                    count_right[person_id] += 1\n",
    "                else:\n",
    "                    cv2.putText(frame, f\"{person_id}: Looking Straight\", (10, 30 + i*30), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 0), 2)\n",
    "                    duration_looking_straight[person_id] += 1 / fps  # Increment the duration\n",
    "\n",
    "            # Draw contours around eyes\n",
    "            for eye in [left_eye, right_eye]:\n",
    "                for point in eye:\n",
    "                    x, y = point[0], point[1]\n",
    "                    cv2.circle(frame, (x, y), 3, (0, 255, 0), -1)\n",
    "\n",
    "        # Emotion detection\n",
    "        (x, y, w, h) = face_utils.rect_to_bb(face)\n",
    "        if w > 0 and h > 0:  # Check if width and height are positive\n",
    "            face_crop = gray[y:y + h, x:x + w]\n",
    "            if face_crop.shape[0] > 0 and face_crop.shape[1] > 0:  # Check if face crop dimensions are positive\n",
    "                face_crop = zoom(face_crop, (48 / face_crop.shape[0], 48 / face_crop.shape[1]))\n",
    "                face_crop = face_crop.astype(np.float32)\n",
    "                face_crop /= float(face_crop.max())\n",
    "                face_crop = np.reshape(face_crop.flatten(), (1, 48, 48, 1))\n",
    "\n",
    "                prediction = emotion_model.predict(face_crop)\n",
    "                prediction_result = np.argmax(prediction)\n",
    "\n",
    "                # Rectangle around the face\n",
    "                cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "                # Annotate main image with emotion label\n",
    "                emotion_labels = [\"Angry\", \"Disgust\", \"Fear\", \"Happy\", \"Sad\", \"Surprise\", \"Neutral\"]\n",
    "                emotion_label = emotion_labels[prediction_result]\n",
    "                cv2.putText(frame, f\"{person_id}: {emotion_label}\", (x + w - 10, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "                emotion_duration[emotion_label.lower()][person_id] += time.time() - emotion_start_time\n",
    "                emotion_start_time = time.time()\n",
    "\n",
    "        # Head pose estimation\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame_rgb.flags.writeable = False\n",
    "        results = face_mesh.process(frame_rgb)\n",
    "        frame_rgb.flags.writeable = True\n",
    "        frame = cv2.cvtColor(frame_rgb, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        img_h, img_w, img_c = frame.shape\n",
    "        face_3d = []\n",
    "        face_2d = []\n",
    "\n",
    "        if results.multi_face_landmarks:\n",
    "            for face_landmarks in results.multi_face_landmarks:\n",
    "                for idx, lm in enumerate(face_landmarks.landmark):\n",
    "                    if idx == 33 or idx == 263 or idx == 1 or idx == 61 or idx == 291 or idx == 199:\n",
    "                        nose_2d = (lm.x * img_w, lm.y * img_h)\n",
    "                        nose_3d = (lm.x * img_w, lm.y * img_h, lm.z * 8000)\n",
    "\n",
    "                    x, y = int(lm.x * img_w), int(lm.y * img_h)\n",
    "\n",
    "                    # Get the 2D Coordinates\n",
    "                    face_2d.append([x, y])\n",
    "\n",
    "                    # Get the 3D Coordinates\n",
    "                    face_3d.append([x, y, lm.z])\n",
    "\n",
    "                face_2d = np.array(face_2d, dtype=np.float64)\n",
    "                face_3d = np.array(face_3d, dtype=np.float64)\n",
    "\n",
    "                # Camera matrix\n",
    "                focal_length = 1 * img_w\n",
    "                cam_matrix = np.array([[focal_length, 0, img_w / 2],\n",
    "                                       [0, focal_length, img_h / 2],\n",
    "                                       [0, 0, 1]])\n",
    "\n",
    "                # Distortion parameters\n",
    "                dist_matrix = np.zeros((4, 1), dtype=np.float64)\n",
    "\n",
    "                # Solve PnP\n",
    "                success, rot_vec, trans_vec = cv2.solvePnP(face_3d, face_2d, cam_matrix, dist_matrix)\n",
    "\n",
    "                # Get rotational matrix\n",
    "                rmat, jac = cv2.Rodrigues(rot_vec)\n",
    "\n",
    "                # Get angles\n",
    "                angles, mtx_r, mtx_q, qx, qy, qz = cv2.RQDecomp3x3(rmat)\n",
    "\n",
    "                # Get the y rotation degree\n",
    "                x_angle = angles[0] * 360\n",
    "                y_angle = angles[1] * 360\n",
    "                z_angle = angles[2] * 360\n",
    "\n",
    "                # See where the user's head tilting\n",
    "                if y_angle < -10:\n",
    "                    text = \"Looking Left\"\n",
    "                    time_left_seconds[person_id] += 1 / fps\n",
    "                elif y_angle > 10:\n",
    "                    text = \"Looking Right\"\n",
    "                    time_right_seconds[person_id] += 1 / fps\n",
    "                elif x_angle < -10:\n",
    "                    text = \"Looking Down\"\n",
    "                    time_down_seconds[person_id] += 1 / fps\n",
    "                elif x_angle > 10:\n",
    "                    text = \"Looking Up\"\n",
    "                    time_up_seconds[person_id] += 1 / fps\n",
    "                else:\n",
    "                    text = \"Looking Forward\"\n",
    "                    time_forward_seconds[person_id] += 1 / fps\n",
    "\n",
    "                # Display the text\n",
    "                cv2.putText(frame, f\"{person_id}: {text}\", (500, 50 + i * 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    # Write the frame to the output video\n",
    "    output_video.write(frame)\n",
    "\n",
    "    # Display the frame in a window\n",
    "    cv2.imshow('Frame', frame)\n",
    "    \n",
    "    # Clear the output in Jupyter notebook\n",
    "    clear_output(wait=True)\n",
    "    display(frame)\n",
    "\n",
    "    # Exit the loop when the 'q' key is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture and writer objects\n",
    "cap.release()\n",
    "output_video.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Write the recorded data to a CSV file\n",
    "with open('eye_tracking_data.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Person ID\", \"Duration Eyes Closed (s)\", \"Duration Looking Left (s)\", \"Duration Looking Right (s)\", \"Duration Looking Straight (s)\", \"Left Counts\", \"Right Counts\", \"Straight Counts\"])\n",
    "    for person_id in duration_eyes_closed:\n",
    "        writer.writerow([person_id, duration_eyes_closed[person_id], duration_looking_left[person_id], duration_looking_right[person_id], duration_looking_straight[person_id], count_left[person_id], count_right[person_id], count_straight[person_id]])\n",
    "\n",
    "with open('emotion_detection_data.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Person ID\", \"Angry (s)\", \"Sad (s)\", \"Happy (s)\", \"Fear (s)\", \"Disgust (s)\", \"Neutral (s)\", \"Surprise (s)\"])\n",
    "    for person_id in emotion_duration[\"angry\"]:\n",
    "        writer.writerow([person_id, emotion_duration[\"angry\"][person_id], emotion_duration[\"sad\"][person_id], emotion_duration[\"happy\"][person_id], emotion_duration[\"fear\"][person_id], emotion_duration[\"disgust\"][person_id], emotion_duration[\"neutral\"][person_id], emotion_duration[\"surprise\"][person_id]])\n",
    "\n",
    "with open('head_pose_data.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Person ID\", \"Looking Forward (s)\", \"Looking Left (s)\", \"Looking Right (s)\", \"Looking Up (s)\", \"Looking Down (s)\"])\n",
    "    for person_id in time_forward_seconds:\n",
    "        writer.writerow([person_id, time_forward_seconds[person_id], time_left_seconds[person_id], time_right_seconds[person_id], time_up_seconds[person_id], time_down_seconds[person_id]])\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a7a538-19a3-4f60-8a33-593a45d2dee9",
   "metadata": {},
   "source": [
    "###NEW"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47fb5ae0-6632-4274-93d2-9dc7429d61bb",
   "metadata": {},
   "source": [
    "import cv2\n",
    "import dlib\n",
    "import numpy as np\n",
    "from IPython.display import display, clear_output\n",
    "from scipy.spatial import distance\n",
    "from imutils import face_utils\n",
    "from scipy.ndimage import zoom\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "import mediapipe as mp\n",
    "import time\n",
    "import csv\n",
    "\n",
    "# Custom deserialization functions\n",
    "def custom_variance_scaling_initializer(config):\n",
    "    if 'dtype' in config:\n",
    "        del config['dtype']\n",
    "    return tf.keras.initializers.VarianceScaling(**config)\n",
    "\n",
    "def custom_zeros_initializer(config):\n",
    "    if 'dtype' in config:\n",
    "        del config['dtype']\n",
    "    return tf.keras.initializers.Zeros()\n",
    "\n",
    "# Register the custom deserialization functions\n",
    "tf.keras.utils.get_custom_objects().update({\n",
    "    'VarianceScaling': custom_variance_scaling_initializer,\n",
    "    'Zeros': custom_zeros_initializer\n",
    "})\n",
    "\n",
    "# Function to detect eyes in a frame\n",
    "def detect_eyes(frame, shape):\n",
    "    left_eye = shape[36:42]\n",
    "    right_eye = shape[42:48]\n",
    "    return left_eye, right_eye\n",
    "\n",
    "# Function to calculate Eye Aspect Ratio (EAR)\n",
    "def calculate_ear(eye):\n",
    "    eye = np.array([(point[0], point[1]) for point in eye])\n",
    "    A = np.linalg.norm(eye[1] - eye[5])\n",
    "    B = np.linalg.norm(eye[2] - eye[4])\n",
    "    C = np.linalg.norm(eye[0] - eye[3])\n",
    "    ear = (A + B) / (2.0 * C)\n",
    "    return ear\n",
    "\n",
    "# Load dlib face detector and facial landmarks predictor\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"C:/Users/nisha/Downloads/brainy-bits/brainy-bits-ai/notebooks/shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "# Load emotion detection model\n",
    "emotion_model = load_model(\"C:/Users/nisha/Downloads/brainy-bits/brainy-bits-ai/notebooks/video.h5\")\n",
    "\n",
    "# Initialize video capture from the camera\n",
    "cap = cv2.VideoCapture(0)  # 0 corresponds to the default camera (you can change it if you have multiple cameras)\n",
    "\n",
    "# Get video properties for the output video\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# Initialize video writer for the output video\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "output_video = cv2.VideoWriter('/content/outputvideo.avi', fourcc, fps, (width, height))\n",
    "\n",
    "# Initialize variables to record durations\n",
    "duration_eyes_closed = {}\n",
    "duration_looking_left = {}\n",
    "duration_looking_right = {}\n",
    "duration_looking_straight = {}\n",
    "\n",
    "# Initialize variables for counting eye movement\n",
    "count_left = {}\n",
    "count_right = {}\n",
    "count_straight = {}\n",
    "\n",
    "# Variables to track emotion detected\n",
    "emotion_start_time = time.time()\n",
    "emotion_duration = {\"angry\": {}, \"sad\": {}, \"happy\": {}, \"fear\": {}, \"disgust\": {}, \"neutral\": {}, \"surprise\": {}}\n",
    "\n",
    "# Variables to track time spent in different head pose directions\n",
    "time_forward_seconds = {}\n",
    "time_left_seconds = {}\n",
    "time_right_seconds = {}\n",
    "time_up_seconds = {}\n",
    "time_down_seconds = {}\n",
    "\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = detector(gray)\n",
    "\n",
    "    for i, face in enumerate(faces):\n",
    "        shape = predictor(gray, face)\n",
    "        shape = face_utils.shape_to_np(shape)\n",
    "\n",
    "        person_id = f\"Person {i+1}\"\n",
    "\n",
    "        if person_id not in duration_eyes_closed:\n",
    "            duration_eyes_closed[person_id] = 0\n",
    "            duration_looking_left[person_id] = 0\n",
    "            duration_looking_right[person_id] = 0\n",
    "            duration_looking_straight[person_id] = 0\n",
    "            count_left[person_id] = 0\n",
    "            count_right[person_id] = 0\n",
    "            count_straight[person_id] = 0\n",
    "            time_forward_seconds[person_id] = 0\n",
    "            time_left_seconds[person_id] = 0\n",
    "            time_right_seconds[person_id] = 0\n",
    "            time_up_seconds[person_id] = 0\n",
    "            time_down_seconds[person_id] = 0\n",
    "            for emotion in emotion_duration:\n",
    "                emotion_duration[emotion][person_id] = 0\n",
    "\n",
    "        # Eye tracking\n",
    "        left_eye, right_eye = detect_eyes(frame, shape)\n",
    "\n",
    "        if left_eye is not None and right_eye is not None:\n",
    "            ear_left = calculate_ear(left_eye)\n",
    "            ear_right = calculate_ear(right_eye)\n",
    "\n",
    "            # Calculate the average EAR for both eyes\n",
    "            avg_ear = (ear_left + ear_right) / 2.0\n",
    "\n",
    "            # Set a threshold for distraction detection (you may need to adjust this)\n",
    "            distraction_threshold = 0.2\n",
    "\n",
    "            # Check if the person is distracted\n",
    "            if avg_ear < distraction_threshold:\n",
    "                cv2.putText(frame, f\"{person_id}: Eyes Closed\", (10, 30 + i*30), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 0, 255), 2)\n",
    "                duration_eyes_closed[person_id] += 1 / fps  # Increment the duration\n",
    "                count_straight[person_id] += 1\n",
    "\n",
    "            else:\n",
    "                # Check gaze direction\n",
    "                horizontal_ratio = (left_eye[0][0] + right_eye[3][0]) / 2 / width\n",
    "                if horizontal_ratio < 0.4:\n",
    "                    cv2.putText(frame, f\"{person_id}: Looking Left\", (10, 30 + i*30), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 0), 2)\n",
    "                    duration_looking_left[person_id] += 1 / fps  # Increment the duration\n",
    "                    count_left[person_id] += 1\n",
    "                elif horizontal_ratio > 0.6:\n",
    "                    cv2.putText(frame, f\"{person_id}: Looking Right\", (10, 30 + i*30), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 0), 2)\n",
    "                    duration_looking_right[person_id] += 1 / fps  # Increment the duration\n",
    "                    count_right[person_id] += 1\n",
    "                else:\n",
    "                    cv2.putText(frame, f\"{person_id}: Looking Straight\", (10, 30 + i*30), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 0), 2)\n",
    "                    duration_looking_straight[person_id] += 1 / fps  # Increment the duration\n",
    "\n",
    "            # Draw contours around eyes\n",
    "            for eye in [left_eye, right_eye]:\n",
    "                for point in eye:\n",
    "                    x, y = point[0], point[1]\n",
    "                    cv2.circle(frame, (x, y), 3, (0, 255, 0), -1)\n",
    "\n",
    "        # Emotion detection\n",
    "        (x, y, w, h) = face_utils.rect_to_bb(face)\n",
    "        if w > 0 and h > 0:  # Check if width and height are positive\n",
    "            face_crop = gray[y:y + h, x:x + w]\n",
    "            if face_crop.shape[0] > 0 and face_crop.shape[1] > 0:  # Check if face crop dimensions are positive\n",
    "                face_crop = zoom(face_crop, (48 / face_crop.shape[0], 48 / face_crop.shape[1]))\n",
    "                face_crop = face_crop.astype(np.float32)\n",
    "                face_crop /= float(face_crop.max())\n",
    "                face_crop = np.reshape(face_crop.flatten(), (1, 48, 48, 1))\n",
    "\n",
    "                prediction = emotion_model.predict(face_crop)\n",
    "                prediction_result = np.argmax(prediction)\n",
    "\n",
    "                # Rectangle around the face\n",
    "                cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "                # Annotate main image with emotion label\n",
    "                emotion_labels = [\"Angry\", \"Disgust\", \"Fear\", \"Happy\", \"Sad\", \"Surprise\", \"Neutral\"]\n",
    "                emotion_label = emotion_labels[prediction_result]\n",
    "                cv2.putText(frame, f\"{person_id}: {emotion_label}\", (x + w - 10, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "                emotion_duration[emotion_label.lower()][person_id] += time.time() - emotion_start_time\n",
    "                emotion_start_time = time.time()\n",
    "\n",
    "        # Head pose estimation\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame_rgb.flags.writeable = False\n",
    "        results = face_mesh.process(frame_rgb)\n",
    "        frame_rgb.flags.writeable = True\n",
    "        frame = cv2.cvtColor(frame_rgb, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        img_h, img_w, img_c = frame.shape\n",
    "        face_3d = []\n",
    "        face_2d = []\n",
    "\n",
    "        if results.multi_face_landmarks:\n",
    "            for face_landmarks in results.multi_face_landmarks:\n",
    "                for idx, lm in enumerate(face_landmarks.landmark):\n",
    "                    if idx == 33 or idx == 263 or idx == 1 or idx == 61 or idx == 291 or idx == 199:\n",
    "                        nose_2d = (lm.x * img_w, lm.y * img_h)\n",
    "                        nose_3d = (lm.x * img_w, lm.y * img_h, lm.z * 8000)\n",
    "\n",
    "                    x, y = int(lm.x * img_w), int(lm.y * img_h)\n",
    "\n",
    "                    # Get the 2D Coordinates\n",
    "                    face_2d.append([x, y])\n",
    "\n",
    "                    # Get the 3D Coordinates\n",
    "                    face_3d.append([x, y, lm.z])\n",
    "\n",
    "                face_2d = np.array(face_2d, dtype=np.float64)\n",
    "                face_3d = np.array(face_3d, dtype=np.float64)\n",
    "\n",
    "                # Camera matrix\n",
    "                focal_length = 1 * img_w\n",
    "                cam_matrix = np.array([[focal_length, 0, img_w / 2],\n",
    "                                       [0, focal_length, img_h / 2],\n",
    "                                       [0, 0, 1]])\n",
    "\n",
    "                # Distortion parameters\n",
    "                dist_matrix = np.zeros((4, 1), dtype=np.float64)\n",
    "\n",
    "                # Solve PnP\n",
    "                success, rot_vec, trans_vec = cv2.solvePnP(face_3d, face_2d, cam_matrix, dist_matrix)\n",
    "\n",
    "                # Get rotational matrix\n",
    "                rmat, jac = cv2.Rodrigues(rot_vec)\n",
    "\n",
    "                # Get angles\n",
    "                angles, mtx_r, mtx_q, qx, qy, qz = cv2.RQDecomp3x3(rmat)\n",
    "\n",
    "                # Get the y rotation degree\n",
    "                x_angle = angles[0] * 360\n",
    "                y_angle = angles[1] * 360\n",
    "                z_angle = angles[2] * 360\n",
    "\n",
    "                # See where the user's head tilting\n",
    "                if y_angle < -10:\n",
    "                    text = \"Looking Left\"\n",
    "                    time_left_seconds[person_id] += 1 / fps\n",
    "                elif y_angle > 10:\n",
    "                    text = \"Looking Right\"\n",
    "                    time_right_seconds[person_id] += 1 / fps\n",
    "                elif x_angle < -10:\n",
    "                    text = \"Looking Down\"\n",
    "                    time_down_seconds[person_id] += 1 / fps\n",
    "                elif x_angle > 10:\n",
    "                    text = \"Looking Up\"\n",
    "                    time_up_seconds[person_id] += 1 / fps\n",
    "                else:\n",
    "                    text = \"Looking Forward\"\n",
    "                    time_forward_seconds[person_id] += 1 / fps\n",
    "\n",
    "                # Display the text\n",
    "                cv2.putText(frame, f\"{person_id}: {text}\", (500, 50 + i * 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    # Write the frame to the output video\n",
    "    output_video.write(frame)\n",
    "\n",
    "    # Display the frame in a window\n",
    "    cv2.imshow('Frame', frame)\n",
    "    \n",
    "    # Clear the output in Jupyter notebook\n",
    "    clear_output(wait=True)\n",
    "    display(frame)\n",
    "\n",
    "    # Exit the loop when the 'q' key is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture and writer objects\n",
    "cap.release()\n",
    "output_video.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Write the recorded data to a CSV file\n",
    "with open('eye_tracking_data.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Person ID\", \"Duration Eyes Closed (s)\", \"Duration Looking Left (s)\", \"Duration Looking Right (s)\", \"Duration Looking Straight (s)\", \"Left Counts\", \"Right Counts\", \"Straight Counts\"])\n",
    "    for person_id in duration_eyes_closed:\n",
    "        writer.writerow([person_id, duration_eyes_closed[person_id], duration_looking_left[person_id], duration_looking_right[person_id], duration_looking_straight[person_id], count_left[person_id], count_right[person_id], count_straight[person_id]])\n",
    "\n",
    "with open('emotion_detection_data.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Person ID\", \"Angry (s)\", \"Sad (s)\", \"Happy (s)\", \"Fear (s)\", \"Disgust (s)\", \"Neutral (s)\", \"Surprise (s)\"])\n",
    "    for person_id in emotion_duration[\"angry\"]:\n",
    "        writer.writerow([person_id, emotion_duration[\"angry\"][person_id], emotion_duration[\"sad\"][person_id], emotion_duration[\"happy\"][person_id], emotion_duration[\"fear\"][person_id], emotion_duration[\"disgust\"][person_id], emotion_duration[\"neutral\"][person_id], emotion_duration[\"surprise\"][person_id]])\n",
    "\n",
    "with open('head_pose_data.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Person ID\", \"Looking Forward (s)\", \"Looking Left (s)\", \"Looking Right (s)\", \"Looking Up (s)\", \"Looking Down (s)\"])\n",
    "    for person_id in time_forward_seconds:\n",
    "        writer.writerow([person_id, time_forward_seconds[person_id], time_left_seconds[person_id], time_right_seconds[person_id], time_up_seconds[person_id], time_down_seconds[person_id]])\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02e9d0b-40c8-4b18-a446-4553016d1f81",
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
