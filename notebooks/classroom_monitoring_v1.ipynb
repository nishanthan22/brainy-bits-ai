{
 "cells": [
  {
   "cell_type": "code",
   "id": "1c7475c5-9ec9-4d3b-9e2e-74b591d59325",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T19:35:19.613369Z",
     "start_time": "2024-07-09T19:35:19.610921Z"
    }
   },
   "source": [
    "print(\"The following cell has the emotion detection algorithm\")\n",
    "# UNIT 1, Working perfectly"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T19:43:37.697089Z",
     "start_time": "2024-07-09T19:43:36.942710Z"
    }
   },
   "source": [
    "\n",
    "import cv2\n",
    "import dlib\n",
    "import numpy as np\n",
    "from IPython.display import display, clear_output\n",
    "from scipy.spatial import distance\n",
    "from imutils import face_utils\n",
    "from scipy.ndimage import zoom\n",
    "from tensorflow.keras.models import load_model\n",
    "import mediapipe as mp\n",
    "import time\n",
    "import csv\n",
    "\n",
    "# Function to detect eyes in a frame\n",
    "def detect_eyes(frame):\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = detector(gray)\n",
    "\n",
    "    if faces:\n",
    "        shape = predictor(gray, faces[0])\n",
    "        left_eye = shape.parts()[36:42]\n",
    "        right_eye = shape.parts()[42:48]\n",
    "        return left_eye, right_eye\n",
    "    else:\n",
    "        return None, None\n",
    "\n",
    "# Function to calculate Eye Aspect Ratio (EAR)\n",
    "def calculate_ear(eye):\n",
    "    eye = np.array([(point.x, point.y) for point in eye])\n",
    "    A = np.linalg.norm(eye[1] - eye[5])\n",
    "    B = np.linalg.norm(eye[2] - eye[4])\n",
    "    C = np.linalg.norm(eye[0] - eye[3])\n",
    "    ear = (A + B) / (2.0 * C)\n",
    "    return ear\n",
    "\n",
    "# Load dlib face detector and facial landmarks predictor\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"../scripts/shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "# Load emotion detection model\n",
    "emotion_model = load_model('../scripts/video.h5')\n",
    "\n",
    "emotion_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Initialize video capture from the camera\n",
    "cap = cv2.VideoCapture(0)  # 0 corresponds to the default camera (you can change it if you have multiple cameras)\n",
    "\n",
    "# Get video properties for the output video\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# Initialize video writer for the output video\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "output_video = cv2.VideoWriter('../results/outputvideo.avi', fourcc, fps, (width, height))\n",
    "\n",
    "# Initialize variables to record durations\n",
    "duration_eyes_closed = 0\n",
    "duration_looking_left = 0\n",
    "duration_looking_right = 0\n",
    "duration_looking_straight = 0\n",
    "\n",
    "# Initialize variables for counting eye movement\n",
    "count_left = 0\n",
    "count_right = 0\n",
    "count_straight = 0\n",
    "\n",
    "# Load face detector and shape predictor for emotion detection\n",
    "face_detector = dlib.get_frontal_face_detector()\n",
    "shape_predictor = dlib.shape_predictor(\"../scripts/face_landmarks.dat\")\n",
    "\n",
    "# Initialize head pose estimation\n",
    "official_start_time = time.time()\n",
    "start_time = time.time()\n",
    "end_time = 0\n",
    "\n",
    "#Variables to track emotion detected\n",
    "emotion_start_time = time.time()\n",
    "e_start_time = time.time()\n",
    "e_end_time = 0\n",
    "angry_emotion = 0\n",
    "sad_emotion = 0\n",
    "happy_emotion = 0\n",
    "fear_emotion = 0\n",
    "disgust_emotion = 0\n",
    "neutral_emotion = 0\n",
    "surprise_emotion = 0\n",
    "\n",
    "# Variables to track time spent in different head pose directions\n",
    "time_forward_seconds = 0\n",
    "time_left_seconds = 0\n",
    "time_right_seconds = 0\n",
    "time_up_seconds = 0\n",
    "time_down_seconds = 0\n",
    "\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Eye tracking\n",
    "    left_eye, right_eye = detect_eyes(frame)\n",
    "\n",
    "    if left_eye is not None and right_eye is not None:\n",
    "        ear_left = calculate_ear(left_eye)\n",
    "        ear_right = calculate_ear(right_eye)\n",
    "\n",
    "        # Calculate the average EAR for both eyes\n",
    "        avg_ear = (ear_left + ear_right) / 2.0\n",
    "\n",
    "        # Set a threshold for distraction detection (you may need to adjust this)\n",
    "        distraction_threshold = 0.2\n",
    "\n",
    "        # Check if the person is distracted\n",
    "        if avg_ear < distraction_threshold:\n",
    "            cv2.putText(frame, \"Eyes Closed\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 0, 255), 2)\n",
    "            duration_eyes_closed += 1 / fps  # Increment the duration\n",
    "            count_straight += 1\n",
    "\n",
    "        else:\n",
    "            # Check gaze direction\n",
    "            horizontal_ratio = (left_eye[0].x + right_eye[3].x) / 2 / width\n",
    "            if horizontal_ratio < 0.4:\n",
    "                cv2.putText(frame, \"Looking Left\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 0), 2)\n",
    "                duration_looking_left += 1 / fps  # Increment the duration\n",
    "                count_left += 1\n",
    "            elif horizontal_ratio > 0.6:\n",
    "                cv2.putText(frame, \"Looking Right\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 0), 2)\n",
    "                duration_looking_right += 1 / fps  # Increment the duration\n",
    "                count_right += 1\n",
    "            else:\n",
    "                cv2.putText(frame, \"Looking Straight\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 0), 2)\n",
    "                duration_looking_straight += 1 / fps  # Increment the duration\n",
    "\n",
    "        # Draw contours around eyes\n",
    "        for eye in [left_eye, right_eye]:\n",
    "            for point in eye:\n",
    "                x, y = point.x, point.y\n",
    "                cv2.circle(frame, (x, y), 3, (0, 255, 0), -1)\n",
    "\n",
    "    # Emotion detection\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    rects = face_detector(gray, 1)\n",
    "\n",
    "    for (i, rect) in enumerate(rects):\n",
    "        shape = shape_predictor(gray, rect)\n",
    "        shape = face_utils.shape_to_np(shape)\n",
    "\n",
    "        (x, y, w, h) = face_utils.rect_to_bb(rect)\n",
    "        face = gray[y:y + h, x:x + w]\n",
    "        face = zoom(face, (48 / face.shape[0], 48 / face.shape[1]))\n",
    "        face = face.astype(np.float32)\n",
    "        face /= float(face.max())\n",
    "        face = np.reshape(face.flatten(), (1, 48, 48, 1))\n",
    "\n",
    "        prediction = emotion_model.predict(face)\n",
    "        prediction_result = np.argmax(prediction)\n",
    "\n",
    "        # Rectangle around the face\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "        # Annotate main image with emotion label\n",
    "        if prediction_result == 0:\n",
    "            cv2.putText(frame, \"Angry\", (x + w - 10, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            angry_emotion += time.time() - e_start_time\n",
    "            e_start_time = time.time()\n",
    "        elif prediction_result == 1:\n",
    "            cv2.putText(frame, \"Disgust\", (x + w - 10, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            disgust_emotion += time.time() - e_start_time\n",
    "            e_start_time = time.time()\n",
    "        elif prediction_result == 2:\n",
    "            cv2.putText(frame, \"Fear\", (x + w - 10, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            fear_emotion += time.time() - e_start_time \n",
    "            e_start_time = time.time()\n",
    "        elif prediction_result == 3:\n",
    "            cv2.putText(frame, \"Happy\", (x + w - 10, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            happy_emotion += time.time() - e_start_time \n",
    "            e_start_time = time.time()\n",
    "        elif prediction_result == 4:\n",
    "            cv2.putText(frame, \"Sad\", (x + w - 10, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            sad_emotion += time.time() - e_start_time \n",
    "            e_start_time = time.time()\n",
    "        elif prediction_result == 5:\n",
    "            cv2.putText(frame, \"Surprise\", (x + w - 10, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            surprise_emotion += time.time() - e_start_time \n",
    "            e_start_time = time.time()\n",
    "        else:\n",
    "            cv2.putText(frame, \"Neutral\", (x + w - 10, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            neutral_emotion += time.time() - e_start_time \n",
    "            e_start_time = time.time()\n",
    "\n",
    "    # Head pose estimation\n",
    "    startTime = time.time()\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) #it was 1\n",
    "#     frame = cv2.cvtColor(cv2.flip(frame, 1), cv2.COLOR_BGR2RGB)\n",
    "    frame.flags.writeable = False\n",
    "    results = face_mesh.process(frame)\n",
    "    frame.flags.writeable = True\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    img_h, img_w, img_c = frame.shape\n",
    "    face_3d = []\n",
    "    face_2d = []\n",
    "\n",
    "    if results.multi_face_landmarks:\n",
    "        for face_landmarks in results.multi_face_landmarks:\n",
    "            for idx, lm in enumerate(face_landmarks.landmark):\n",
    "                if idx == 33 or idx == 263 or idx == 1 or idx == 61 or idx == 291 or idx == 199:\n",
    "                    if idx == 1:\n",
    "                        nose_2d = (lm.x * img_w, lm.y * img_h)\n",
    "                        nose_3d = (lm.x * img_w, lm.y * img_h, lm.z * 8000)\n",
    "\n",
    "                    x, y = int(lm.x * img_w), int(lm.y * img_h)\n",
    "\n",
    "                    # Get the 2D Coordinates\n",
    "                    face_2d.append([x, y])\n",
    "\n",
    "                    # Get the 3D Coordinates\n",
    "                    face_3d.append([x, y, lm.z])\n",
    "\n",
    "            face_2d = np.array(face_2d, dtype=np.float64)\n",
    "            face_3d = np.array(face_3d, dtype=np.float64)\n",
    "            focal_length = 1 * img_w\n",
    "\n",
    "            cam_matrix = np.array([[focal_length, 0, img_h / 2],\n",
    "                                   [0, focal_length, img_w / 2],\n",
    "                                   [0, 0, 1]])\n",
    "\n",
    "            dist_matrix = np.zeros((4, 1), dtype=np.float64)\n",
    "            success, rot_vec, trans_vec = cv2.solvePnP(face_3d, face_2d, cam_matrix, dist_matrix)\n",
    "            rmat, jac = cv2.Rodrigues(rot_vec)\n",
    "\n",
    "            angles, mtxR, mtxQ, Qx, Qy, Qz = cv2.RQDecomp3x3(rmat)\n",
    "\n",
    "            print(f\"X Rotation: {angles[0] * 10000}\")\n",
    "            print(f\"Y Rotation: {angles[1] * 10000}\")\n",
    "\n",
    "            if angles[1] * 10000 < -100:\n",
    "                text = \"Looking Left\"\n",
    "                time_left_seconds += time.time() - start_time\n",
    "                start_time = time.time()\n",
    "\n",
    "            elif angles[1] * 10000 > 100:\n",
    "                text = \"Looking Right\"\n",
    "                time_right_seconds += time.time() - start_time\n",
    "                start_time = time.time()\n",
    "\n",
    "            elif angles[0] * 10000 < -100:\n",
    "                text = \"Looking Down\"\n",
    "                time_down_seconds += time.time() - start_time\n",
    "                start_time = time.time()\n",
    "\n",
    "            elif angles[0] * 10000 > 200:\n",
    "                text = \"Looking Up\"\n",
    "                time_up_seconds += time.time() - start_time\n",
    "                start_time = time.time()\n",
    "\n",
    "            else:\n",
    "                text = \"Forward\"\n",
    "                time_forward_seconds += time.time() - start_time\n",
    "                start_time = time.time()\n",
    "\n",
    "            # Display the nose direction\n",
    "            nose_3d_projection, jacobian = cv2.projectPoints(nose_3d, rot_vec, trans_vec, cam_matrix, dist_matrix)\n",
    "\n",
    "            p1 = (int(nose_2d[0]), int(nose_2d[1]))\n",
    "            p2 = (int(nose_3d_projection[0][0][0]), int(nose_3d_projection[0][0][1]))\n",
    "\n",
    "            cv2.line(frame, p1, p2, (255, 0, 0), 2)\n",
    "\n",
    "            cv2.putText(frame, text, (width - 250, 30), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 0, 255), 2)\n",
    "\n",
    "    # Open the CSV file in write mode and append the angles to it\n",
    "    with open('headPoses.csv', mode='a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "\n",
    "        # Write the header row if the file is empty\n",
    "        if file.tell() == 0:\n",
    "            writer.writerow([\"X Rotation\", \"Y Rotation\"])\n",
    "\n",
    "        # Write the angles to the CSV file\n",
    "        #writer.writerow([angles[0] * 10000, angles[1] * 10000]) #bonbon\n",
    "\n",
    "    output_video.write(frame)  # Write the frame to the output video\n",
    "\n",
    "    # Display the frame without modifying color\n",
    "    cv2.imshow('Frame', frame)\n",
    "    # Clear the previous output\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture object, video writer, and close all windows\n",
    "cap.release()\n",
    "output_video.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "# Print the durations and most observed features for emotion detection\n",
    "print(f\"\\nEmotion Detection:\")\n",
    "print(f\"Duration of Happiness: {happy_emotion} seconds\")\n",
    "print(f\"Duration of Sadness: {sad_emotion} seconds\")\n",
    "print(f\"Duration of Disgust: {disgust_emotion} seconds\")\n",
    "print(f\"Duration of Fear: {fear_emotion} seconds\")\n",
    "print(f\"Duration of Anger: {angry_emotion} seconds\")\n",
    "print(f\"Duration of Neutral: {neutral_emotion} seconds\")\n",
    "print(f\"Duration of Surprise: {surprise_emotion} seconds\")\n",
    "\n",
    "# Determine the most observed emotions movement\n",
    "max_eye_duration = max(happy_emotion, sad_emotion, disgust_emotion, fear_emotion, angry_emotion, neutral_emotion, surprise_emotion)\n",
    "if max_eye_duration == happy_emotion:\n",
    "    print(\"The most observed emotion: Happiness\")\n",
    "elif max_eye_duration == sad_emotion:\n",
    "    print(\"The most observed emotion: Sadness\")\n",
    "elif max_eye_duration == disgust_emotion:\n",
    "    print(\"The most observed emotion: Disgust\")\n",
    "elif max_eye_duration == fear_emotion:\n",
    "    print(\"The most observed emotion: Fear\")\n",
    "elif max_eye_duration == angry_emotion:\n",
    "    print(\"The most observed emotion: Anger\")\n",
    "elif max_eye_duration == surprise_emotion:\n",
    "    print(\"The most observed emotion: Surprise\")\n",
    "else:\n",
    "    print(\"The most observed emotion: Neutral\")\n",
    "\n",
    "\n",
    "# Print the durations and most observed features for eyes\n",
    "print(f\"\\nEye Movements:\")\n",
    "print(f\"Duration taken looking right: {duration_looking_right} sec\")\n",
    "print(f\"Duration taken closed eyes: {duration_eyes_closed} sec\")\n",
    "print(f\"Duration taken looking left: {duration_looking_left} sec\")\n",
    "print(f\"Duration taken looking straight: {duration_looking_straight} sec\")\n",
    "\n",
    "# Determine the most observed eye movement\n",
    "max_eye_duration = max(duration_looking_right, duration_eyes_closed, duration_looking_left, duration_looking_straight)\n",
    "if max_eye_duration == duration_looking_right:\n",
    "    print(\"The most observed eye movement: Looking Right\")\n",
    "elif max_eye_duration == duration_eyes_closed:\n",
    "    print(\"The most observed eye movement: Eyes Closed\")\n",
    "elif max_eye_duration == duration_looking_left:\n",
    "    print(\"The most observed eye movement: Looking Left\")\n",
    "else:\n",
    "    print(\"The most observed eye movement: Looking Straight\")\n",
    "\n",
    "# Print the durations and most observed features for head pose\n",
    "print(f\"\\nHead Pose Estimation:\")\n",
    "print(f\"Duration of Time Looking Forward: {time_forward_seconds} seconds\")\n",
    "print(f\"Duration of Time Looking Up: {time_up_seconds} seconds\")\n",
    "print(f\"Duration of Time Looking Left: q{time_left_seconds} seconds\")\n",
    "print(f\"Duration of Time Looking Right: {time_right_seconds} seconds\")\n",
    "print(f\"Duration of Time Looking Down: {time_down_seconds} seconds\")\n",
    "\n",
    "# Determine the most observed eye movement\n",
    "max_eye_duration = max(time_forward_seconds, time_up_seconds, time_left_seconds, time_right_seconds, time_down_seconds)\n",
    "if max_eye_duration == time_forward_seconds:\n",
    "    print('\\033[93m'+\"The most observed head pose: Facing Forward\")\n",
    "elif max_eye_duration == time_up_seconds:\n",
    "    print('\\033[93m'+\"The most observed head pose: Facing Upwards\")\n",
    "elif max_eye_duration == time_left_seconds:\n",
    "    print('\\033[93m'+\"The most observed head pose: Facing Left\")\n",
    "elif max_eye_duration == time_right_seconds:\n",
    "    print('\\033[93m'+\"The most observed head pose: Facing Right\")\n",
    "else:\n",
    "    print('\\033[93m'+\"The most observed head pose: Facing Downwards\")\n"
   ],
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "8812bfdb7783986f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T19:53:35.698386Z",
     "start_time": "2024-07-09T19:53:35.695629Z"
    }
   },
   "source": [
    "print(\"The following cell has multiple face detection algorithm\")\n",
    "# UNIT 2, Working perfectly"
   ],
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "cc2fd3b33d18729",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T19:54:20.130056Z",
     "start_time": "2024-07-09T19:53:39.601278Z"
    }
   },
   "source": [
    "import cv2\n",
    "import dlib\n",
    "\n",
    "# Initialize the video capture (use 0 for the first camera device)\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Initialize dlib's face detector\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "\n",
    "person_count = 1\n",
    "\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convert the frame to grayscale\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect faces in the grayscale frame\n",
    "    faces = detector(gray)\n",
    "\n",
    "    # Loop through each detected face\n",
    "    for i, face in enumerate(faces):\n",
    "        # Draw a rectangle around the face\n",
    "        x, y, w, h = (face.left(), face.top(), face.width(), face.height())\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "\n",
    "        # Label the face with a unique identifier\n",
    "        label = f'Person {i+1}'\n",
    "        cv2.putText(frame, label, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('Face Detection', frame)\n",
    "\n",
    "    # Break the loop on 'q' key press\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the capture and destroy all OpenCV windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ],
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "07f859b5-7510-4e5c-b858-c2394020996a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T19:54:41.515343Z",
     "start_time": "2024-07-09T19:54:41.512126Z"
    }
   },
   "source": [
    "# Test code 1\n",
    "print(\"Simple code to read the .h5 file\")"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "ab20e00f-ded7-4ead-8813-7a3413f01d51",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T19:55:04.873281Z",
     "start_time": "2024-07-09T19:55:04.726567Z"
    }
   },
   "source": [
    "import h5py\n",
    "\n",
    "# Path to the HDF5 file\n",
    "h5_file_path = '../scripts/video_modified.h5'\n",
    "\n",
    "# Open the HDF5 file for reading\n",
    "with h5py.File(h5_file_path, 'r') as f:\n",
    "    print(\"Keys in the HDF5 file:\")\n",
    "    print(\"======================\")\n",
    "    # Print all groups and datasets\n",
    "    def print_attrs(name, obj):\n",
    "        print(name)\n",
    "        for key, val in obj.attrs.items():\n",
    "            print(\"    %s: %s\" % (key, val))\n",
    "\n",
    "    f.visititems(print_attrs)\n"
   ],
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "edf4f9447fe1678b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T19:55:09.205167Z",
     "start_time": "2024-07-09T19:55:09.202615Z"
    }
   },
   "source": [
    "# FINAL, Test success\n",
    "print(\"The following cell has the integrated code which is final one of version 1\")"
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "0e8bd206-752f-4206-a47c-a881219c9b2f",
   "metadata": {},
   "source": [
    "!pip3 install opencv-python-headless\n",
    "!pip3 install dlib\n",
    "!pip3 install numpy\n",
    "!pip3 install ipython\n",
    "!pip3 install scipy\n",
    "!pip3 install imutils\n",
    "!pip3 install tensorflow\n",
    "!pip3 install mediapipe\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import dlib\n",
    "import numpy as np\n",
    "from IPython.display import display, clear_output\n",
    "from scipy.spatial import distance\n",
    "from imutils import face_utils\n",
    "from scipy.ndimage import zoom\n",
    "from tensorflow.keras.models import load_model\n",
    "import mediapipe as mp\n",
    "import time\n",
    "import csv\n",
    "import tensorflow as tf\n",
    "\n",
    "# Custom deserialization function for VarianceScaling\n",
    "def custom_VarianceScaling_deserializer(config):\n",
    "    from tensorflow.keras.initializers import VarianceScaling\n",
    "    # Remove 'dtype' from config if it exists\n",
    "    config.pop('dtype', None)\n",
    "    return VarianceScaling(**config)\n",
    "\n",
    "# Register the custom deserializer\n",
    "tf.keras.utils.get_custom_objects().update({'VarianceScaling': custom_VarianceScaling_deserializer})\n",
    "\n",
    "# Function to detect eyes in a frame\n",
    "def detect_eyes(frame, shape):\n",
    "    left_eye = shape[36:42]\n",
    "    right_eye = shape[42:48]\n",
    "    return left_eye, right_eye\n",
    "\n",
    "# Function to calculate Eye Aspect Ratio (EAR)\n",
    "def calculate_ear(eye):\n",
    "    eye = np.array([(point[0], point[1]) for point in eye])\n",
    "    A = np.linalg.norm(eye[1] - eye[5])\n",
    "    B = np.linalg.norm(eye[2] - eye[4])\n",
    "    C = np.linalg.norm(eye[0] - eye[3])\n",
    "    ear = (A + B) / (2.0 * C)\n",
    "    return ear\n",
    "\n",
    "# Function to get the specified file's path\n",
    "def get_abs_path(directory, file):\n",
    "    directory_path = os.path.join(os.getcwd(), '..', directory)\n",
    "    file_path = os.path.join(directory_path, file)\n",
    "    return file_path\n",
    "\n",
    "# Load dlib face detector and facial landmarks predictor\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(get_abs_path('scripts', 'shape_predictor_68_face_landmarks.dat'))\n",
    "\n",
    "# Load emotion detection model\n",
    "emotion_model = load_model(get_abs_path('scripts', 'FER_model.h5'))\n",
    "\n",
    "emotion_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Initialize video capture from the camera\n",
    "cap = cv2.VideoCapture(0)  # 0 corresponds to the default camera (you can change it if you have multiple cameras)\n",
    "\n",
    "# Get video properties for the output video\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# Initialize video writer for the output video\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "output_video = cv2.VideoWriter(get_abs_path('results', 'outputvideo.avi'), fourcc, fps, (width, height))\n",
    "\n",
    "# Initialize variables to record durations\n",
    "duration_eyes_closed = {}\n",
    "duration_looking_left = {}\n",
    "duration_looking_right = {}\n",
    "duration_looking_straight = {}\n",
    "\n",
    "# Initialize variables for counting eye movement\n",
    "count_left = {}\n",
    "count_right = {}\n",
    "count_straight = {}\n",
    "\n",
    "# Variables to track emotion detected\n",
    "emotion_start_time = time.time()\n",
    "emotion_duration = {\"angry\": {}, \"sad\": {}, \"happy\": {}, \"fear\": {}, \"disgust\": {}, \"neutral\": {}, \"surprise\": {}}\n",
    "\n",
    "# Variables to track time spent in different head pose directions\n",
    "time_forward_seconds = {}\n",
    "time_left_seconds = {}\n",
    "time_right_seconds = {}\n",
    "time_up_seconds = {}\n",
    "time_down_seconds = {}\n",
    "\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = detector(gray)\n",
    "\n",
    "    for i, face in enumerate(faces):\n",
    "        shape = predictor(gray, face)\n",
    "        shape = face_utils.shape_to_np(shape)\n",
    "\n",
    "        person_id = f\"Person {i+1}\"\n",
    "\n",
    "        if person_id not in duration_eyes_closed:\n",
    "            duration_eyes_closed[person_id] = 0\n",
    "            duration_looking_left[person_id] = 0\n",
    "            duration_looking_right[person_id] = 0\n",
    "            duration_looking_straight[person_id] = 0\n",
    "            count_left[person_id] = 0\n",
    "            count_right[person_id] = 0\n",
    "            count_straight[person_id] = 0\n",
    "            time_forward_seconds[person_id] = 0\n",
    "            time_left_seconds[person_id] = 0\n",
    "            time_right_seconds[person_id] = 0\n",
    "            time_up_seconds[person_id] = 0\n",
    "            time_down_seconds[person_id] = 0\n",
    "            for emotion in emotion_duration:\n",
    "                emotion_duration[emotion][person_id] = 0\n",
    "\n",
    "        # Eye tracking\n",
    "        left_eye, right_eye = detect_eyes(frame, shape)\n",
    "\n",
    "        if left_eye is not None and right_eye is not None:\n",
    "            ear_left = calculate_ear(left_eye)\n",
    "            ear_right = calculate_ear(right_eye)\n",
    "\n",
    "            # Calculate the average EAR for both eyes\n",
    "            avg_ear = (ear_left + ear_right) / 2.0\n",
    "\n",
    "            # Set a threshold for distraction detection (you may need to adjust this)\n",
    "            distraction_threshold = 0.2\n",
    "\n",
    "            # Check if the person is distracted\n",
    "            if avg_ear < distraction_threshold:\n",
    "                cv2.putText(frame, f\"{person_id}: Eyes Closed\", (10, 30 + i*30), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 0, 255), 2)\n",
    "                duration_eyes_closed[person_id] += 1 / fps  # Increment the duration\n",
    "                count_straight[person_id] += 1\n",
    "\n",
    "            else:\n",
    "                # Check gaze direction\n",
    "                horizontal_ratio = (left_eye[0][0] + right_eye[3][0]) / 2 / width\n",
    "                if horizontal_ratio < 0.4:\n",
    "                    cv2.putText(frame, f\"{person_id}: Looking Left\", (10, 30 + i*30), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 0), 2)\n",
    "                    duration_looking_left[person_id] += 1 / fps  # Increment the duration\n",
    "                    count_left[person_id] += 1\n",
    "                elif horizontal_ratio > 0.6:\n",
    "                    cv2.putText(frame, f\"{person_id}: Looking Right\", (10, 30 + i*30), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 0), 2)\n",
    "                    duration_looking_right[person_id] += 1 / fps  # Increment the duration\n",
    "                    count_right[person_id] += 1\n",
    "                else:\n",
    "                    cv2.putText(frame, f\"{person_id}: Looking Straight\", (10, 30 + i*30), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 0), 2)\n",
    "                    duration_looking_straight[person_id] += 1 / fps  # Increment the duration\n",
    "\n",
    "            # Draw contours around eyes\n",
    "            for eye in [left_eye, right_eye]:\n",
    "                for point in eye:\n",
    "                    x, y = point[0], point[1]\n",
    "                    cv2.circle(frame, (x, y), 3, (0, 255, 0), -1)\n",
    "\n",
    "        # Emotion detection\n",
    "        (x, y, w, h) = face_utils.rect_to_bb(face)\n",
    "        face_crop = gray[y:y + h, x:x + w]\n",
    "        face_crop = zoom(face_crop, (48 / face_crop.shape[0], 48 / face_crop.shape[1]))\n",
    "        face_crop = face_crop.astype(np.float32)\n",
    "        face_crop /= float(face_crop.max())\n",
    "        face_crop = np.reshape(face_crop.flatten(), (1, 48, 48, 1))\n",
    "\n",
    "        prediction = emotion_model.predict(face_crop)\n",
    "        prediction_result = np.argmax(prediction)\n",
    "\n",
    "        # Rectangle around the face\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "        # Annotate main image with emotion label\n",
    "        emotion_labels = [\"Angry\", \"Disgust\", \"Fear\", \"Happy\", \"Sad\", \"Surprise\", \"Neutral\"]\n",
    "        emotion_label = emotion_labels[prediction_result]\n",
    "        cv2.putText(frame, f\"{person_id}: {emotion_label}\", (x + w - 10, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "        emotion_duration[emotion_label.lower()][person_id] += time.time() - emotion_start_time\n",
    "        emotion_start_time = time.time()\n",
    "\n",
    "        # Head pose estimation\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame_rgb.flags.writeable = False\n",
    "        results = face_mesh.process(frame_rgb)\n",
    "        frame_rgb.flags.writeable = True\n",
    "        frame = cv2.cvtColor(frame_rgb, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        img_h, img_w, img_c = frame.shape\n",
    "        face_3d = []\n",
    "        face_2d = []\n",
    "\n",
    "        if results.multi_face_landmarks:\n",
    "          for face_landmarks in results.multi_face_landmarks:\n",
    "              for idx, lm in enumerate(face_landmarks.landmark):\n",
    "                  if idx == 33 or idx == 263 or idx == 1 or idx == 61 or idx == 291 or idx == 199:\n",
    "                      if idx == 1:\n",
    "                          nose_2d = (lm.x * img_w, lm.y * img_h)\n",
    "                          nose_3d = (lm.x * img_w, lm.y * img_h, lm.z * 8000)\n",
    "                      x, y = int(lm.x * img_w), int(lm.y * img_h)\n",
    "\n",
    "                      # Get the 2D Coordinates\n",
    "                      face_2d.append([x, y])\n",
    "\n",
    "                      # Get the 3D Coordinates\n",
    "                      face_3d.append([x, y, lm.z])\n",
    "\n",
    "          face_2d = np.array(face_2d, dtype=np.float64)\n",
    "          face_3d = np.array(face_3d, dtype=np.float64)\n",
    "\n",
    "          # Camera matrix\n",
    "          focal_length = 1 * img_w\n",
    "          cam_matrix = np.array([[focal_length, 0, img_w / 2],\n",
    "                                [0, focal_length, img_h / 2],\n",
    "                                [0, 0, 1]])\n",
    "\n",
    "          # Distortion parameters\n",
    "          dist_matrix = np.zeros((4, 1), dtype=np.float64)\n",
    "\n",
    "          # Solve PnP\n",
    "          success, rot_vec, trans_vec = cv2.solvePnP(face_3d, face_2d, cam_matrix, dist_matrix)\n",
    "\n",
    "          # Get rotational matrix\n",
    "          rmat, jac = cv2.Rodrigues(rot_vec)\n",
    "\n",
    "          # Get angles\n",
    "          angles, mtx_r, mtx_q, qx, qy, qz = cv2.RQDecomp3x3(rmat)\n",
    "\n",
    "          # Get the y rotation degree\n",
    "          x_angle = angles[0] * 360\n",
    "          y_angle = angles[1] * 360\n",
    "          z_angle = angles[2] * 360\n",
    "\n",
    "          # See where the user's head tilting\n",
    "          if y_angle < -10:\n",
    "              text = \"Looking Left\"\n",
    "              time_left_seconds[person_id] += 1 / fps\n",
    "          elif y_angle > 10:\n",
    "              text = \"Looking Right\"\n",
    "              time_right_seconds[person_id] += 1 / fps\n",
    "          elif x_angle < -10:\n",
    "              text = \"Looking Down\"\n",
    "              time_down_seconds[person_id] += 1 / fps\n",
    "          elif x_angle > 10:\n",
    "              text = \"Looking Up\"\n",
    "              time_up_seconds[person_id] += 1 / fps\n",
    "          else:\n",
    "              text = \"Looking Forward\"\n",
    "              time_forward_seconds[person_id] += 1 / fps\n",
    "\n",
    "          # Display the text\n",
    "          cv2.putText(frame, f\"{person_id}: {text}\", (500, 50 + i * 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    # Write the frame to the output video\n",
    "    output_video.write(frame)\n",
    "\n",
    "    # Display the frame in a window\n",
    "    cv2.imshow('Frame', frame)\n",
    "\n",
    "    # Clear the output in Jupyter notebook\n",
    "    clear_output(wait=True)\n",
    "    display(frame)\n",
    "\n",
    "    # Exit the loop when the 'q' key is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "output_video.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "with open(get_abs_path('results', 'eye_tracking_data.csv'), 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Person ID\", \"Duration Eyes Closed (s)\", \"Duration Looking Left (s)\", \"Duration Looking Right (s)\", \"Duration Looking Straight (s)\", \"Left Counts\", \"Right Counts\", \"Straight Counts\"])\n",
    "    for person_id in duration_eyes_closed:\n",
    "        writer.writerow([person_id, duration_eyes_closed[person_id], duration_looking_left[person_id], duration_looking_right[person_id], duration_looking_straight[person_id], count_left[person_id], count_right[person_id], count_straight[person_id]])\n",
    "\n",
    "with open(get_abs_path('results', 'emotion_detection_data.csv'), 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Person ID\", \"Angry (s)\", \"Sad (s)\", \"Happy (s)\", \"Fear (s)\", \"Disgust (s)\", \"Neutral (s)\", \"Surprise (s)\"])\n",
    "    for person_id in emotion_duration[\"angry\"]:\n",
    "        writer.writerow([person_id, emotion_duration[\"angry\"][person_id], emotion_duration[\"sad\"][person_id], emotion_duration[\"happy\"][person_id], emotion_duration[\"fear\"][person_id], emotion_duration[\"disgust\"][person_id], emotion_duration[\"neutral\"][person_id], emotion_duration[\"surprise\"][person_id]])\n",
    "\n",
    "with open(get_abs_path('results', 'head_pose_data.csv'), 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Person ID\", \"Looking Forward (s)\", \"Looking Left (s)\", \"Looking Right (s)\", \"Looking Up (s)\", \"Looking Down (s)\"])\n",
    "    for person_id in time_forward_seconds:\n",
    "        writer.writerow([person_id, time_forward_seconds[person_id], time_left_seconds[person_id], time_right_seconds[person_id], time_up_seconds[person_id], time_down_seconds[person_id]])\n"
   ],
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76f40b5-f2b6-4e79-82fc-a800afdc90eb",
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
